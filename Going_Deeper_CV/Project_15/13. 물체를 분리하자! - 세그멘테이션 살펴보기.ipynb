{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMYr9axjMXPJMZUA7ycCDKZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 13-1. 들어가며\n","- 이미지 분류(image classification)\n","- 객체 인식(obejct detection)\n","=> 이미지에서 어떤 물체의 종류를 분류하거나 물체의 존재와 위치를 탐지해 낸다.\n","<br/><br/>\n","- 세그멘테이션(segmentation)은 픽셀 수준에서 이미지의 각 부분이 어떤 의미를 갖는 영역인지 분리해내는 방법이다.\n","- 세그멘테이션은 이미지 분할 기술이다.\n","> 이미지 분할\n","> - 이미지 내에서 객체의 경계를 감지하고 객체를 개별적으로 식별하는 데 사용한다.\n","- 사람의 영역과 배경 영역을 분리해서 배경을 흐리게 처리하여 아웃포커싱 효과를 보여준다.\n","---\n","### 학습 내용\n","- 2. 세그멘테이션 문제의 종류\n","    - 시맨틱 세그멘테이션(semantic segmentation)과 인스턴스 세그멘테이션(instance segmentation)의 특징을 학습한다.\n","- 3. 주요 세그멘테이션 모델 (1) FCN\n","    - FCN의 구조와 특징을 학습한다.\n","\n","- 4. 주요 세그멘테이션 모델 (2) U-Net\n","    - U-Net의 구조와 특징을 학습한다.\n","\n","- 5. 주요 세그멘테이션 모델 (3) DeepLab 계열\n","    - DeepLab의 구조와 특징을 학습한다.\n","\n","- 6. 세그멘테이션의 평가\n","    - 세그멘테이션의 평가지표를 학습한다.\n","\n","- 7. Upsampling의 다양한 방법\n","    - Upsampling 방법론을 학습한다.\n","---"],"metadata":{"id":"ArGlUsvo6SCO"}},{"cell_type":"markdown","source":["## 13-2. 세그멘테이션 문제의 종류\n","- 이미지 내에서 영역을 분리하는 접근 방식은 크게 두 가지 방식이 있다.\n","- 영역을 분리한다는 관점에서 비슷하지만, 접근 방식에 따라 문제의 정의와 모델을 구성하는 방식이 달라진다.\n","<br/>\n","\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/semantic_vs_instance.png)\n","<br/>\n","\n","    1. 시맨틱 세그멘테이션(semantic segmentation)\n","        - 이미지에서 픽셀 단위로 물체나 배경을 분류하는 작업\n","    2. 인스턴스 세그멘테이션(instance segmentation)\n","        - 각 물체를 고유하게 식별하여 분할하는 작업\n","<br/><br/>\n","- 위 사진의 좌상단 사진\n","    - 단지 어떤 물체들이 모여 있는 영역의 위치를 인식(localization)과 이 물체들이 양이라는 것을 판별(classification)하는 접근법\n","<br/><br/>\n","- 위 사진의 우상단 사진\n","    - 비슷하지만 개별 양들의 개체 하나하나의 위치를 정확히 식별하는 객체 인식(object detection)의 접근법\n","<br/><br/>\n","- 위 사진의 좌하단 사진\n","    - 양, 길, 풀밭의 영역을 정확히 구분해낼 뿐 양들 각각의 구분해 내진 않는다. (시맨틱 세그멘테이션)\n","<br/><br/>\n","- 위 사진의 우하단 사진\n","    - 각 양의 개체들의 영역을 픽셀 단위로 정확히 구분해 낸다. (인스턴스 세그멘테이션)\n","---\n","\n"],"metadata":{"id":"acandP9X7iGA"}},{"cell_type":"markdown","source":["### 1) 시맨틱 세그멘테이션\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/u-net.max-800x600.png)\n","- 위 모델 구조는 시맨틱 세그멘테이션의 대표적인 모델인 U-Net의 구조이다.\n","- 입력으로 572x572 크기인 이미지가 input\n","- 388x388의 크기의 output\n","- 2 가지의 클래스를 가진 세그멘테이션 맵이 나온다.\n","    - 2 가지 클래스는 가장 마지막 레이어의 채널 개수가 \"2\"라는 점에서 확인 가능\n","<br/>\n","- 이때 두 가지의 클래스를 문제에 따라 다르게 정의하면 클래스에 따른 시맨틱 세그멘테이션 맵을 얻을 수 있다.\n","    - 인물사진 모드 : 사람의 영역과 배경 클래스\n","    - 의료 인공지능 : 세포 사진에서 병이 있는 영역과 정상인 영역\n","<br/><br/>\n","\n","=> 세그멘테이션을 위해서 이미지의 각 픽셀에 해당하는 영역의 클래스별 정보가 필요하다. 그래서 이미지 분류나 object detection 문제보다 큰 출력값이 나온다.\n","\n","---"],"metadata":{"id":"3N7BFHD644fl"}},{"cell_type":"markdown","source":["### 2) 인스턴스 세그멘테이션\n","- 같은 클래스 내에서도 각 개체(instance)들을 분리하여 세그멘테이션을 수행한다.\n","- obejct detection 모델로 각 개체를 구분하고 이후에 각 개체 별로 시맨틱 세그멘테이션을 수행하면 인스턴스 세그멘테이션을 할 수 있다.\n","<br/><br/>\n","- 이러한 방식 중 대표적인 것은 Mask R-CNN이다.\n","- 2-Stage Object Detection의 가장 대표적인 Faster-R-CNN을 계승한것\n","\n","> 1-Stage Object Detection:\n","> - 단일 단계에서 객체의 bounding box와 클래스를 동시에 예측한다.\n","> - 물체의 위치와 클래스를 하나의 네트워크에서 한 번에 결정하는 방식이다.\n","\n","> 2-Stage Object Detection:\n","> - 두 단계를 거쳐 객체의 위치를 예측한다.\n","> - 먼저 후보 영역(RoI)을 생성하고,\n","> - 그 후에 각 후보 영역(RoI)에 대해 물체의 클래스를 분류하고 경계 박스(Bounding box)를 조정한다.\n","<br/><br/>\n","- Faster-R-CNN의 아이디어인 Region of Interest Pooling Layer(RoIPool) 개념을 개선\n","1. 정확한 Segmentation에 유리하게 한 RoIAlign,\n","2. 클래스별 마스크 분리\n","- 두 가지 아이디어를 통해, 클래스별 Object Detection과 시맨틱 세그멘테이션을 사실상 하나의 Task로 엮어낸 것으로 평가받는 중요한 모델이다.\n","<br/><br/>\n","\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-5-L-FasterRCNN-01.png)\n","- RoIPool Layer는 다양한 RoI 영역을 Pooling을 통해 동일한 크기의 Feature map으로 추출해 내는 레이어이다.\n","    - 고정된 크기의 특성 맵으로 반환하기 위해 Pooling을 사용한다.\n","    - RoIPooling을 통해 후보 객체를 일정한 크기의 입력으로 변환하여 동일한 크기의 특성을 얻을 수 있다.\n","\n","> RoI(후보 영역)\n","> - 이미지에서 추출된 후보 객체의 영역\n","> - 다음과 같은 과정으로 RoI가 얻어진다.\n","> 1. Region Proposal Network(RPN) : 이미지에서 가능성 있는 물체의 위치를 제안하는 역할을 하는 RPN을 사용하여 Bounding box를 생성한다.\n","> 2. RoI Pooling 또는 RoI Align : RPN에서 얻은 후보 객체의 Bounding box를 사용하여 원본 이미지 또는 특성 맵에서 해당 위치에 해당하는 영역을 추출한다. 다음과 같은 기술을 사용하여 고정된 크기의 특성 맵을 얻는다.\n","> - RoI는 이렇게 얻어 고정된 크기의 특성을 가지고 있다.\n","> - RoI는 최종적으로 객체의 특성을 추출하고, 해당 RoI에 대한 객체의 클래스를 분류하고 경계 박스를 조정하는 데 사용된다.\n","> - RoI를 통해 모델은 후보 객체의 영역에 집중하여 객체를 정확하게 감지하고 분류한다.\n","<br/><br/>\n","\n","- RoIPool 과정에서 object 영역의 정확한 마스킹을 필요로하는 Segmentation 문제에서 Quantization(양자화)이 필요하다는 점에서 문제가 발생한다.\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/GC-5-L-RoIPool-01.max-800x600.png)\n","- 위 그림의 예에서는 16x16으로 분할한다.\n","- 이미지에서 RoI 영역(후보 영역)의 크기는 다양한데,\n","- 모든 RoI 영역의 가로/세로 픽셀 크기가 16의 배수인 것은 아니다.\n","- 위 그림의 예에서 가로 200, 세로 145 픽셀짜리 RoI 영역<br/><br/>\n","= 16x16으로 분할된 영역 중 절반 이상이 RoI 영역에 덮이는 곳들로 끼워 맞추다 보면,\n","어쩔 수 없이 RoI 영역 밖이 포함되는 경우도 있고, 자투리 영역이 버려지는 경우도 생긴다.\n","\n","=> 이런 상황은 필연적으로 시맨틱 세그멘테이션의 정보손실과 왜곡을 야기한다.\n","\n","```\n","### 정리 ###\n","- RoIPooling은 특성 맵을 고정된 크기의 그리드로 분할,\n","- 각 그리드 셀에서 최댓값을 추출하여 고정된 크기의 특성 맵을 얻는다.\n","- 그러나 이렇게 크기가 고정된 그리드로 분할하는 과정에서\n","- 원본 RoI 영역의 크기가 그리드 크기의 배수가 아니라면 정보의 손실이 발생한다. (반올림 연산 등의 으로 손실 발생)\n","\n","=> 이러한 정보의 손실을 최소화하기 위해 RoIPooling 대신에 RoIAlign과 같은 기술을 사용하면 됀다.\n","- RoI의 크기에 더 정확하게 맞춰진 특성을 얻기 위해 보간(interpolation)을 사용하여 정보 손실을 감소시킨다.\n","1. Grid Subdivision(그리드 세분화) : RoI를 특성 맵의 그리드에 맞춘다.\n","2. 보간(Interpolation) : RoI 영역 내의 픽셀 값을 얻을 때, 정수 좌표가 아닌 실수 좌표에 대해 선형 보간을 사용한다.\n","    - 예를 들어, RoI의 가로 방향 위치가 3.5라면, 이는 그리드 셀 내에서 3번째와 4번째 픽셀 사이의 위치에 해당한다.\n","    - 선형 보간은 두 인접한 정수 좌표와 값에 가중치를 주어 새로운 좌표에 해당하는 값을 추정하는 방법이다.\n","    - 3번째와 4번째 픽셀의 값을 0.5 : 0.5의 비율로 선형 보간하여 RoI의 위치에 대한 값을 얻는다.\n","\n","=> 작은 RoI 영역이나 정확한 위치가 필요한 경우에는 RoIAlign은 RoIPooling보다 더 나은 정확도를 제공한다.\n","```"],"metadata":{"id":"YuTSAVn452x2"}},{"cell_type":"markdown","source":["- Mask-R-CNN의 RoIAlign은 Quantization(양자화)하지 않고도 RoI를 처리할 고정 사이즈의 Feature map을 생성할 수 있게 아이디어를 제공한다.\n","- RoI 영역을 pooling layer의 크기에 맞추어 등분한 후,\n","- RoIPool을 했을 때의 양자화 영역 중 가까운 것들과의 이중선형보간 계산을 통해 생성해야 할 feature map을 계산한다.\n","\n","> 이중선형보간(bilinear interpolation)\n","> - 2차원 공간에서 주어진 4개의 값에 대해 보간을 수행\n","> - 주어진 2x2 그리드의 네 꼭지점의 값을 (x1, y1), (x1, y2), (x2, y1), (x2, y2)라고 할 때,\n","> - 어떤 지점 (x, y)에 대한 값을 추정한다.\n","> - 먼저 수평 방향으로 선형 보간을 수행한 후,\n","> - 그 결과에 수직 방향으로 선형 보간을 수행한다.\n","\n","```\n","선형 보간은 1차원에서 두 점을 연결하는 직선 상의 값을 추정하는 반면,\n","이중 선형 보간은 2차원에서 2x2 그리드 상의 네 꼭지점 값을 사용하여 새로운 지점의 값을 추정한다.\n","```\n","\n","- Mask-R-CNN은 Faster R-CNN에서 특성 추출방식을 \"RoIAlign\" 방식으로 개선을 하고 세그멘테이션을 더한 방식이다.\n","- U-Net처럼 feature map의 크기를 키워 mask를 생성해 내는 부분을 통해 인스턴스 맵을 추론한다.\n","---"],"metadata":{"id":"ycynbcisEksr"}},{"cell_type":"markdown","source":["## 13-3. 주요 세그멘테이션 모델 (1) FCN\n","> - [FCN 원본 논문](https://arxiv.org/pdf/1411.4038.pdf)\n","> - [FCN 논문 리뷰](https://medium.com/@msmapark2/fcn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-fully-convolutional-networks-for-semantic-segmentation-81f016d76204)\n","---\n","\n","### Fully Convolutional Networks for Semantic Segmentation(FCN)\n","- Semantic Segmentation 문제를 위해 제안된 딥러닝 모델\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*xWh0LhoDzwyjcxXpnz9lIQ.png)\n","- AlexNet, VGG-16 등의 모델을 세그멘테이션에 맞게 변형한 모델\n","- 기본적인 VGG 모델은 이미지의 특성을 추출하기 위해 네트워크 뒷단에 fully connected layer를 붙여서 계산한 클래스별 확률을 바탕으로 이미지 분류를 수행한다.\n","- 하지만 FCN에서는 세그멘테이션을 하기 위해 네트워크 뒷단에 fully connected layer 대신 CNN을 붙여준다.\n","1. Convolutionalization(컨볼루션화)\n","2. Upsampling\n","3. Skip architecture\n","\n","```\n","*** 중요 ***\n","Image classification 모델들은 기본적으로 내부 구조와 관계없이 모델의 근본적인 목표를 위해 출력층이 Fully-connected layer로 구성되어 있다.\n","- 이러한 구성은 네트워크 입력층에서 중간부분까지 ConvNet을 이용하여 영상의 특징들을 추출하고 해당 특징들을 출력층 부분에서 fc를 이용해 이미지를 분류하기 위함이다.\n","- 하지만 이러한 fc layer는 한계가 있다.\n","1. 이미지의 위치 정보가 사라진다.\n","2. 입력 이미지 크기가 고정된다.\n","- Segmenation의 목적은 원본 이미지의 각 픽셀에 대해 클래스를 구분하고 인스턴스 및 배경을 분할하는 것으로 위치 정보가 매우 중요하다.\n","- 그래서 fc-layer를 conv-layer로 대체하는 방법을 택했다.\n","```\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/fcn_2.max-800x600.png)\n","\n","- VGG16을 예로 살펴보자\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*ddf0sJZDitiqVCWdjijzCA.png)\n","<br/><br/>\n","- 마지막 CNN은 1x1의 커널 크기와 클래스의 개수만큼의 채널을 갖는다. 이렇게 CNN을 거치면 클래스 히트맵을 얻을 수 있다.\n","- 하지만 히트맵의 크기는 일반적으로 원본 이미지보다 작다. CNN과 Pooling 레이어를 거치면서 크기가 줄었기 때문이다.\n","= 이를 키워주는 방법을 upsampling이라고 한다.\n","<br/><br/>\n","- Upsampling에는 여러 방법이 있다.\n","- 그중 FCN에서는\n","    1. Transpose Convolution : 컨볼루션 연산을 거꾸로 해준 것\n","    2. Interpolation : 보간법으로 주어진 값들을 통해 추정해야 하는 픽셀(여기서는 특성 맵의 크기가 커지면서 메꾸어야 하는 중간 픽셀들을 의미) 추정\n","\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/fcn_3.max-800x600.png)\n","- Upsampling만 하면 원하는 세그멘테이션 맵을 얻을 수 있는데 그것이 바로 FCN-32s의 경우이다.\n","<br/><br/>\n","- 하지만 더 나은 성능을 위해서 Skip Architecture라는 방법을 더해준다.\n","- 보다 더 정확하고 상세한 구분(Segmenation)을 얻기 위해 사용\n","1. Deep & Coarse(추상적인) 레이어의 의미적 정보\n","    - Deep : 깊은 층\n","    - Coarse : 덜 세분화된, 추상적인 정보\n","    = 높은 수준에서의 의미적인 정보를 담고 있는 층\n","\n","2. Shallow & fine 층의 외관적 정보:\n","    - shallow : 얕은 층\n","    - Fine : 세분화된, 구체적인 정보\n","    = 낮은 수준에서의 외관적인 특징이나 세부적인 시각적 정보(텍스처, 색상, 경계)\n","\n","- Skip architecture는 이러한 깊은(의미적) 층과 얕은(외관적) 층 간의 정보를 결합하는 구조이다.\n","- Skip connection은 네트워크에서 특정 층의 출력을 다른 층에 직접 연결하는 구조\n","- 따라서 깊은 층의 의미적 정보와 얕은 층의 외관적 정보를 효과적으로 결합하기 위해 skip connection 등을 사용하여 네트워크 아키텍처를 설계하거나 정의한다.\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/fcn_4.max-800x600.png)\n","\n","```\n","*** 정리 ***\n","FCN은 기존의 딥러닝 기반 이미지 분류를 위해 학습이 완료된 모델의 구조를 Semantic Sementation 목적에 맞게 수정하여 Transfer learning 하였다.\n","\n","1. Convolutionalization(컨볼루션화)\n","    - 위치 정보를 잃지 않기 위해서 FC 말고 CNN을 마지막 레이어로 사용하였다.\n","\n","2. Transpose Convolution (Upsampling)\n","    - CNN과 Pooling 레이어를 거치면서 원본 이미지보다 작아졌기 때문에\n","    1. Transpose Convolution : 컨볼루션 연산을 거꾸로 해준 것\n","    2. Interpolation : 보간법으로 주어진 값들을 통해 추정해야 하는 픽셀(여기서는 특성 맵의 크기가 커지면서 메꾸어야 하는 중간 픽셀들을 의미) 추정\n","    - 이와 같은 방법을 사용하여 Upsampling을 해준다.\n","\n","3. Skip architecture\n","    - 더 나은 성능을 얻기 위해 깊은(의미적) 층과 얕은 (외관적) 층 간의 정보를 결합하는 구조이다.\n","```\n","---"],"metadata":{"id":"y1LPmb-uHWEs"}},{"cell_type":"markdown","source":["## 13-4. 주요 세그멘테이션 모델 (2) U-Net\n","> - [U-Net 원본 논문](https://arxiv.org/pdf/1505.04597.pdf)\n","> - [U-Net 논문 리뷰](https://medium.com/@msmapark2/u-net-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-u-net-convolutional-networks-for-biomedical-image-segmentation-456d6901b28a)\n","---\n","\n","### U-Net\n","- 의학 분야에서 Image Segmentation을 목적으로 제안된 End-to-End 방식의 Fully-Convolutional Network 기반 모델이다.\n","![image](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*qNdglJ1ORP3Gq77MmBLhHQ.png)\n","- 이미지의 전반적인 컨텍스트 정보를 얻기 위한 네트워크와 정확한 지역화(Localization)를 위한 네트워크가 대칭 형태로 구성되어 있다.\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*i_MUV1KAoILdjS_u6sAfCw.png)\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*4BVF-6Mdpp-Z_4KBbg2dyQ.png)\n","- FCN에서 upsampling을 통해서 특성 맵을 키운 것을 입력값과 대칭적으로 만들어 준 것이다.\n","- U-Net은 FCN을 토대로 확장한 개념으로\n","- 수축 단계(Contracting Path)\n","- 팽창 단계(Expanding Path)\n","- 팽창 단계의 경우 수축 단계의 최종 특징 맵으로부터 보다 높은 해상도의 Segmentation 결과를 얻기 위해 Upsampling을 진행한다.\n","<br/><br/>\n","- 좌측의 Contracting path는 일반적으로 사용한 Convolution network와 유사하다.\n","- 각 블록은 두 개의 3x3 Convolution 계층과 ReLU를 가지고\n","- 그 뒤로 downsampling을 위해서 2x2의 커널을 2stride로 max pooling을 하게된다.\n","- Downsampling을 거친 후 다음 convolution의 채널 크기는 두 배씩 늘어나도록 설계되어있다.\n","<br/><br/>\n","- 우측의 Expansive path에서는 각 블록에 2x2 up-convolution이 붙어 채널이 절반씩 줄어들고 특성 맵의 크기는 늘어난다.\n","- contracting block(contracting path에서 사용한 CNN)과 동일하게 3x3 convolution이 두개씩 사용됀다.\n","<br/><br/>\n","- 두 Path에서 크기가 같은 블록의 출력과 입력은 skip connection처럼 연결해 주어 low-level의 feature를 활용할 수 있도록한다.\n","- 마지막에는 1x1 convolution으로 원하는 시맨틱 세그멘테이션 맵을 얻을 수 있다.\n","- 결과적으로, 입력으로 572x572 크기인 이미지가 들어가고 출력으로 388x388의 크기에 두 가지의 클래스를 가진 세그멘테이션 맵이 나온다.\n","<br/><br/>\n","= 마지막 세그멘테이션 맵의 크기가 입력 이미지와 다른 것은 세그멘테이션 맵을 원하는 크기로 조정하여(resize) 해결할 수 있다.\n","- 원본 이미지에 맞게 크기를 조정해 주면 우리가 원하는 시맨틱 세그멘테이션 결과를 얻을 수 있다.\n","> 모델의 입력과 출력의 크기가 다른 이유는?\n","> - Convolution은 **padding을** 통해서 크기를 같게 유지할 수 있으나,\n","> - U-Net에선 **padding을** 하지 않아서 Transpose Convolution으로 확대하더라도 원래 이미지 크기가 될 수 없다.\n","---"],"metadata":{"id":"hGjO408tPOjc"}},{"cell_type":"markdown","source":["### 타일(Tile) 기법\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/unet.max-800x600.png)\n","- U-Net이 downsampling과 upsampling(또는 인코딩과 디코딩)을 대칭으로 하는 점은 구조에서 한눈에 파악이 된다.\n","- 또 다른 차이점은 세그멘테이션 맵의 해상도이다.\n","- FCN은 입력 이미지의 크기를 조정하여 세그멘테이션 맵을 얻어낸다.\n","- 반면 U-Net은 타일(tile) 방식을 사용해서 어느 정도 서로 겹치는 구간으로 타일을 나누어 네트워크를 추론,\n","- 큰 이미지에서도 높은 해상도의 세그멘테이션 맵을 얻을 수 있도록 했다.\n","---"],"metadata":{"id":"H-Tv51fti8VW"}},{"cell_type":"markdown","source":["### 데이터 불균형 해결\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/unet_2.max-800x600.png)\n","- 세포를 검출해 내기 위해서는 세포들의 영역뿐만 아니라 경계 또는 예측을 해야 한다.\n","- 이때 픽셀 단위로 라벨을 매긴다고 생각하면,\n","- 데이터셋에 세포나 배경보다는 절대적으로 세포 간 경계의 면적이 작을 것이다.\n","- 이러한 클래스 간 데이터 양의 불균형을 해결해 주기 위해서는 분포를 고려한 weight map을 학습 때 사용했다고 한다.\n","<br/><br/>\n","- weight map의 weight를 신경망의 학습 파라미터를 가리키는 weight가 아니다.\n","- 실제로 여기서 말하는 weight는 손실 함수(loss)에 적용되는 가중치를 말한다.\n","- 의료 영상에서 세포 내부나 배경보다는 상대적으로 면적이 작은 세포 경계를 명확하게 추론해 내는 것이 더욱 중요하기 때문에,\n","- 세포 경계의 손실에 더 많은 패널티를 부과하는 방식이다.\n","---"],"metadata":{"id":"kyMzhgxUjhfD"}},{"cell_type":"markdown","source":["## 13-5. 주요 세그멘테이션 모델 (3) DeepLab 계\n","> - [DeepLab 원본 논문](https://arxiv.org/pdf/1802.02611.pdf)\n","> - [DeepLab 논문 리뷰](https://medium.com/hyunjulie/2%ED%8E%B8-%EB%91%90-%EC%A0%91%EA%B7%BC%EC%9D%98-%EC%A0%91%EC%A0%90-deeplab-v3-ef7316d4209d)\n","---\n","### DeepLab\n","- DeepLabv3+는 이전의 많은 버전을 거쳐 개선을 이뤄온 네트워크이다.\n","- 처음 DeepLab 모델이 제안된 뒤 이 모델을 개선하기 위해 Atrous Convolution와 Spatial Pyramid Pooling 등 많은 방법들이 제안되어 왔다.\n","\n","> Atrous Convolution\n","> - convolution 연산에서 필터의 간격을 넓혀, 확장시켜 이미지의 특성을 고려하는 방법 다른 이름으로 dilated convolution이라고도 한다.\n","\n","> Spatial Pyramid Pooling\n","> - 여러 가지 스케일로 convolution과 pooling을 하고 나온 다양한 특성을 연결(concatenate)해 준다.\n","---\n","### 인코더 & 디코더 구조\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*QTSA3QeDwt-Hk-DZpDIeEg.png)\n","- 인코더\n","    - 입력 데이터로부터 유용한 정보를 추출하여 특성을 만든다.\n","    - 입력 데이터를 점진적으로 다운샘플링하면서 중요한 정보를 요약하고, 추상화한다.(CNN)\n","\n","- 디코더\n","    - 인코더에서 추출한 특성을 이용하여 최종 출력을 생성한다.\n","    - 인코더에서 얻은 특성을 업샘플링하고, 원래 입력과 유사한 형태로 복원하거나 목표에 맞는 출력을 생성한다.\n","\n","- 인코더-디코더\n","    - 인코더에서 추출된 특성은 디코더로 전달되어 최종 출력을 생성하는데 사용된다.\n","---\n","### Atrous Convolution\n","![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9_9NViWHzMVtgYPL1StFWA.png)\n","- A trous(구멍이 있는 : 프랑스어)\n","- 일반적인 convolution 사이에 공간을 넣어서 구멍이 뚤린 듯 한 구조이다.\n","- 같은 연산량으로 더 큰 특징을 잡아낼 수 있다.\n","- 다양한 확장 비율을 가진 atrous convolution을 병렬적으로 사용해서 더 많은 특징을 담을 수 있다.\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/atrous_conv.gif)\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/atrous_conv_2.gif)\n","- 간단히 말하면 \"띄엄띄엄 보는 컨볼루션\"\n","- 더 넓은 영역을 보도록 해주기 위한 방법으로 커널이 일정 간격으로 떨어져 있다.\n","- 이를 통해 컨볼루션 레이어를 너무 깊게 쌓지 않아도 넓은 영역의 정보를 커버할 수 있다.\n","---\n","### 전체 구조\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/deeplab_v3.max-800x600.png)\n","- U-Net에서의 Contracting path과 Expansive path의 역할을 하는 것이 여기서는 인코더(Encoder), 디코더(Decoder)이다.\n","- 인코더는 이미지에서 필요한 정보를 특성으로 추출해 내는 모듈이고\n","- 디코더는 추출된 특성을 이용해 원하는 정보를 예측하는 모듈\n","<br/><br/>\n","- 3x3 convolution을 사용했던 U-Net과 달리\n","- DeepLabV3+는 Atrous Conovolution을 사용한다.\n","- Atrous Convolution을 여러 크기에 다양하게 적용한 것이 ASPP(Atrous Spatial Pyramid Pooling)이다.\n","<br/><br/>\n","- DeepLabV3+는 ASPP가 있는 블록을 통해 특성을 추출하고 디코더에서 Upsampling을 통해 세그멘테이션 마스크를 얻고 있다.\n","\n","> 세그멘테이션 마스크\n","> - 이미지의 각 픽셀을 특정 클래스 또는 객체로 레이블링한 것으로, 픽셀 수준에서 객체의 경계를 정확하게 정의\n","---\n","### Spatial Pyramid Pooling\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/GC-5-L-SPP.max-800x600.png)\n","- Spatial Pyramid Pooling은 여러 가지 스케일로 Convolution과 Pooling을 하고 나온 다양한 특성을 연결(concatenate)해 준다.\n","- 이를 통해서 멀티스케일로 특성을 추출하는 것을 병렬로 수행하는 효과를 얻을 수 있다.\n","- 여기서 컨볼루션을 Atrous Convolution으로 바꾸어 적용한 것은 Atrous Spatial Pyramid Pooling이라고 한다.\n","- 이러한 아키텍쳐는 입력 이미지의 크기와 관계없이 동일한 구조를 활용할 수 있다는 장점이 있다.\n","- 그러므로 제각기 다양한 크기와 비율을 가진 RoI 영역에 대해 적용하기에 유리하다.\n","---"],"metadata":{"id":"hd4Ju4MhktDw"}},{"cell_type":"markdown","source":["## 13-6. 세그멘테이션 평가\n","[한번쯤 읽어봐라](https://www.jeremyjordan.me/evaluating-image-segmentation-models/)\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/segmentation_metric.max-800x600.png)\n","- 만들어진 시맨틱 세그멘테이션 모델을 평가하는 평가 지표를 알아보자.\n","<br/><br/>\n","- 일반적으로 시맨틱 세그멘테이션의 결괏값은\n","- 이미지의 크기에 맞는 세그멘테이션 맵 크기와\n","- 시맨틱 클래스의 수에 맞는 채널 크기\n","---\n","### 1) 픽셀별 정확도(Pixel Accuracy)\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/error_metric.max-800x600.jpg)\n","- 픽셀에 따른 정확도\n","- 세그멘테이션 문제를 픽셀에 따른 이미지 분류 문제로 생각했을 때,\n","- 이미지 분류 문제와 비슷하게 픽셀별 분류 정확도를 세그멘테이션 모델을 평가하는 기준으로 생각할 수 있다.\n","<br/><br/>\n","- 이때 예측 결과 맵(prediction map)을 클래스 별로 평가하는 경우에는 이진 분류 문제로 생각해 픽셀 및 채널 별로 평가한다.\n","- 픽셀 별 이미지 분류 문제로 평가하는 경우에는 픽셀 별로 정답 클래스를 맞추었는지 여부,\n","- 즉 True/False를 구분한다.\n","<br/><br/>\n","- Pixel Accuracy = 맞게 예측된 픽셀 수 / 전체 픽셀 수\n","- 예를 들어 5개의 픽셀 중에서 4개의 픽셀이 정답과 일치하다면\n","= Pixel Accuracy = 4/5 = 0.8\n","- 이는 80%의 픽셀이 정확하게 예측되었다는 것을 의미\n","---\n","### 2) 마스크 IoU(Mask Intersection-over-Union)\n","- object detection 모델을 평가할 때\n","- 정답 라벨(ground truth)와 예측 결과 바운딩 박스(prediction bounding box) 사이의 IoU(intersection over union)를 사용\n","-마스크도 일종의 영역임을 생각했을 때 세그멘테이션 문제에서는 정답인 영역과 예측한 영역의 IoU를 계산할 수 있다.\n","- 아래 식은 세그멘테이션 마스크의 IoU 계산식이다.\n","```\n","# sample for mask iou\n","intersection = np.logical_and(target, prediction)\n","union = np.logical_or(target, prediction)\n","iou_score = np.sum(intersection) / np.sum(union)\n","```\n","- 마스크 IoU를 클래스 별로 계산하면 한 이미지에서 여러 클래스에 대한 IoU 점수를 얻을 수 있다.\n","- 이를 평균하면 전체적인 시맨틱 세그멘테이션 성능을 가늠할 수 있다.\n","\n","> IoU\n","> - 모델이 예측한 영역과 실제 영역간의 겹침을 측정\n","> - IoU = 예측 영역과 실제 영역의 교집합 영역 / 예측 영역과 실제 영역의 합집합 영역\n","> - 바운딩 박스의 겹침 정도를 나타내는 값을 얻는다.\n","> - 0~1 사이의 값을 가지게 되고, IoU가 1에 가까울수록 모델의 예측이 정확하다고 할 수 있다.\n","> - IoU가 낮을수록 예측이 부정확하다는 것을 의미\n","---"],"metadata":{"id":"9TCGLadxp6D9"}},{"cell_type":"markdown","source":["## 13-7. Upsampling의 다양한 방법\n","- Convolution layer와 다양한 Pooling등으로 Feature의 크기를 줄여왔는데, 반대로 키우는 방법에는 어떤 방법들이 있을까?\n","\n","### 1) Nearest Neighbor\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/upsampling1.png)\n","- Nearest upsampling은 이름 그대로 scale을 키운 위치에서 원본에서 가장 가까운 값을 그대로 적용하는 방법이다.\n","- 위 그림처럼 2x2 matrix가 있을 때 이를 2배로 키우면 4x4 matrix가 된다.\n","- 이때 좌측 상단으로부터 2x2는 입력 matrix의 1x1과 가장 가깝다.\n","- 따라서 해당 값을 그대로 사용하게 된다.\n","---\n","### 2) Bilinear Interpolation\n","![image](https://d3s0tskafalll9.cloudfront.net/media/original_images/bi_interpolation.png)\n","- 두 축에 대해서 선형보간법을 통해 필요한 값을 메우는 방식이다.\n","- 2x2 matrix를 4x4로 upsampling을 할 때 위의 이미지처럼 빈 값을을 채워야한다.\n","- 선형보간법을 사용하는 것인데 이때 축을 두 방향으로 활용하기 때문에 Bilinear Interpolation이라고 한다.\n","<br/><br/>\n","- 위 그림에서 두 가지 interpolation을 적용한 것을 순서대로 확인할 수 있다.\n","1. R_1 : Q_11과 Q_21의 x축방향의 interpolation\n","- R_2 : Q_12와 Q_22의 x축방향의 interpolation\n","2. P : R_1과 R_2의 y축방향의 interpolation\n","<br/><br/>\n","- 이러한 선형 보간 방법으로 Interpolation을 하는 방법이 있고 Bicubic interpolation의 경우 삼차보간법을 사용한다.\n","---\n","### 3) Transposed Convolution\n","![image](https://d3s0tskafalll9.cloudfront.net/media/images/transposed_conv.max-800x600.jpg)\n","- 학습할 수 있는 파라미터를 가진 Upsampling 방법이다.\n","- Convolution Layer는 Kernel의 크기를 정의하고 입력된 Feature를 Window에 따라서 output을 계산한다.\n","- Transposed Convolution은 이와 반대로 연산한다.\n","- 거꾸로 학습된 파라미터로 입력된 벡터를 통해 더 넓은 영역의 값을 추정해낸다.\n","\n","> Convolution\n","> - 입력 이미지의 크기를 줄이고 정보를 추상화하는 작업으로서 다운샘플링을 수행한다.\n","> - Convolution은 학습 가능한 파라미터인 필터의 가중치를 사용하여 입력 데이터의 특징을 추출한다.<br/>\n","\n","> Transposed Convolution\n","> - Convoluation의 역과정을 모방하여 입력과 출력의 공간적인 관계를 반대로 만든다.\n","> - 주로 업샘플링 작업에 사용되며, 특징 맵의 크기를 확장한다.\n","> - 기본적으로 학습 가능한 파라미터를 가지며, 역전파 과정에서 역함수 역할을 수행한다.\n","\n"],"metadata":{"id":"LOU_CtNy55iB"}}]}