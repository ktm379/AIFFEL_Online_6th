{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install summa"
      ],
      "metadata": {
        "id": "CEmDK7HBqpW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvoramdVTvar"
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "import nltk\n",
        "import tensorflow\n",
        "import summa\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 데이터 수집하기\n",
        "- 데이터는 아래 링크에 있는 뉴스 기사 데이터(news_summary_more.csv)를 사용한다.\n",
        "- [뉴스 기사 데이터](https://github.com/sunnysai12345/News_Summary)"
      ],
      "metadata": {
        "id": "P-NRwE6hUVAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
        "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "tG_uDLVnUAxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(10)"
      ],
      "metadata": {
        "id": "to8gRZyrUNl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이 데이터는 기사의 본문에 해당되는 text와 headlines 두 가지 열로 구성되어져 있다.\n",
        "- 추상적 요약을 하는 경우에는 text를 본문, headlines를 이미 요약된 데이터로 삼아서 모델을 학습할 수 있다.\n",
        "- 추출적 요약을 하는 경우에는 오직 text열만을 사용한다."
      ],
      "metadata": {
        "id": "fHTwm7JqUz_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 데이터 전처리하기 (추상적 요약)\n",
        "- 실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 추가 사용하여 텍스틑 정규화 또는 정제해 보세요.\n",
        "- 만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 불용어를 제거하는 것이 좋을지 고민해 보세요."
      ],
      "metadata": {
        "id": "uadgr9_XUX0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 중복 샘플 제거\n",
        "- 유일값 개수 세기 : `pandas.unique()`\n",
        "- 중복 제거 : pandas.drop_duplicates()\n",
        "<br/><br/>\n",
        "- text 중복은 제거해야 한다.\n",
        "- text가 달라도 headlines는 같을 수 있어서 중복 제거를 하지 않는다."
      ],
      "metadata": {
        "id": "q5WJEaAEWxs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
        "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
      ],
      "metadata": {
        "id": "4RUL1V5mWMMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
        "data.drop_duplicates(subset = ['text'], inplace=True)\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "tpwzvCZtXHQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Null 값 제거하기\n",
        "- `drop_dulication()`이 중복된 Null들을 지워주기는 하지만, 여전히 Null 값 한 개가 어딘가 남아 있을 것이기 때문에 `.isnull().sum()`을 사용하여 확인할 수 있다.\n",
        "- 이를 `dropna()` 함수를 사용하여 제거한다."
      ],
      "metadata": {
        "id": "Lu1Fe3WlXMP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "eTTmgVvkXiuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 하지만 이 데이터는 Null값이 존재하지 않다."
      ],
      "metadata": {
        "id": "3LL6N-TVXmFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 정규화와 불용어 제거\n",
        "- 같은 의미의 단어들을 같은 표현으로 통일시키기\n",
        "- 기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있는 방법이다.\n",
        "- 이러한 방법론을 텍스트 처리에서는 텍스트 정규화(text normalization)이라고 한다."
      ],
      "metadata": {
        "id": "gs_5W3U8XxuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "print(\"정규화 사전의 수: \", len(contractions))"
      ],
      "metadata": {
        "id": "GgNWwatdYKXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트에는 자주 등장하지만 자연어 처리를 할 때 실질적으로 별 도움이 되지 않는 단어들을 불용어(stopwords)라고 한다.\n",
        "- NLTK에서 제공하는 불용어 리스트를 참고해, 샘플에서 불용어를 제거할 수 있다."
      ],
      "metadata": {
        "id": "S58XaA-NYM2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print('불용어 개수 :', len(stopwords.words('english') ))\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "hSbwHyRsYdux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모든 영어 문자를 소문자로\n",
        "- 섞여 있는 HTML 태그 제거\n",
        "- 정규 표현식을 통해 각종 특수문자 제거\n",
        "- 불용어 제거 : [text] 전처리에서만 사용, [headlines]는 요약 결과문이 자연스러운 문장이 되려면 불용어 제거를 하지 않는 것이 좋다.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfSDL-IPY3jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# 데이터 전처리 함수\n",
        "def preprocess_sentence(sentence, remove_stopwords=True):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
        "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
        "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
        "\n",
        "    # 불용어 제거 (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
        "    # 불용어 미제거 (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "FUATf7PrYdsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다.\n",
        "clean_text = []\n",
        "\n",
        "# [[YOUR CODE]]\n",
        "for text in data['text']:\n",
        "    clean_text.append(preprocess_sentence(text))\n",
        "\n",
        "# 전처리 후 출력\n",
        "print(\"Text 전처리 후 결과: \", clean_text[:5])"
      ],
      "metadata": {
        "id": "KJkiUla9Ydps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 headlines 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다.\n",
        "clean_headlines = []\n",
        "\n",
        "# [[YOUR CODE]]\n",
        "for headline in data['headlines']:\n",
        "    clean_headlines.append(preprocess_sentence(headline, remove_stopwords=False))\n",
        "\n",
        "print(\"Headlines 전처리 후 결과: \", clean_headlines[:5])"
      ],
      "metadata": {
        "id": "g4ZKdn46YdnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트 정제의 과정을 거친 후에는 다시 한번 empty 데이터가 있는지 확인.\n",
        "    - empty 데이터 모두 Null 값으로 대체\n",
        "    - Null 데이터 제거."
      ],
      "metadata": {
        "id": "OR5Shcu1b7Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data['text'] = clean_text\n",
        "data['headlines'] = clean_headlines\n",
        "\n",
        "# 빈 값을 Null 값으로 변환\n",
        "data.replace('', np.nan, inplace=True)\n",
        "\n",
        "# Null값이 생겼는지 확인\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "xnYw9GceYda7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기에서는 다행이 Null 값이 없기 때문에 정제가 완료된 전체 데이터를 확인해보자."
      ],
      "metadata": {
        "id": "W2jgc3orcpsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정제 후 전체 데이터 확인\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "RHwPFUYVcyS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시작 토큰과 종료 토큰 추가하기\n",
        "- 학습에 사용할 데이터의 크기를 결정하고, 문장의 시작과 끝을 토큰으로 표시하기."
      ],
      "metadata": {
        "id": "5uTJxWI-c_zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이 분포 출력\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "text_len = [len(s.split()) for s in data['text']]\n",
        "headlines_len = [len(s.split()) for s in data['headlines']]\n",
        "\n",
        "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
        "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
        "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
        "print('요약의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
        "print('요약의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
        "print('요약의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(text_len)\n",
        "plt.title('Text')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(headlines_len)\n",
        "plt.title('Headlines')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Text Length Violin Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.violinplot(y=text_len, inner=\"quartile\")\n",
        "plt.title('Text Length Violin Plot')\n",
        "plt.ylabel('Length of Samples')\n",
        "\n",
        "# Headlines Length Violin Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.violinplot(y=headlines_len, inner=\"quartile\")\n",
        "plt.title('Headlines Length Violin Plot')\n",
        "plt.ylabel('Length of Samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jtSLu2IjdKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- text는 대체적으로 40 내외의 길이를 가진다.\n",
        "- headlines는 대체적으로 11 이하의 길이를 가진다."
      ],
      "metadata": {
        "id": "S4mHS6r_fzWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 40  # text 최대 길이 설정\n",
        "headlines_max_len = 11  # headlines 최대 길이 설정"
      ],
      "metadata": {
        "id": "PdYv16x_d4jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터의 길이가 데이터의 몇 %에 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해보자."
      ],
      "metadata": {
        "id": "Gz8KFwSYgahL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s.split()) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
      ],
      "metadata": {
        "id": "SUv15eiUgWQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "below_threshold_len(text_max_len, data['text'])\n",
        "below_threshold_len(headlines_max_len, data['headlines'])"
      ],
      "metadata": {
        "id": "-S0K0u3hgWOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제해보자."
      ],
      "metadata": {
        "id": "TjtQQ8HxgwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
        "\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "KIG8C6QUgWL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있다.\n",
        "- 시작 토큰은 `sostoken`, 종료 토큰은 `eostoken`이라 임의로 명명하고 앞, 뒤로 추가해보자."
      ],
      "metadata": {
        "id": "cKrFL8iVg-Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
        "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
        "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ELZ8ve5jgWJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장해보자."
      ],
      "metadata": {
        "id": "wGWBiOKnhR4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = np.array(data['text']) # 인코더의 입력\n",
        "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
        "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
      ],
      "metadata": {
        "id": "YtJC5If4gWGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 데이터와 테스트 데이터로 분리하기.\n",
        "- 데이터를 분리하는 방법\n",
        "    1. 분리 패키지를 사용하는 방법\n",
        "    2. 직접 코딩을 통해서 분리하는 방법"
      ],
      "metadata": {
        "id": "C4EGX82FhcwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input과 크기와 형태가 같은 순서가 섞인 정수 시퀀스 만들기\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# 정수 시퀀스로 데이터의 샘플 순서를 정의해 주면 잘 섞인 샘플이 된다.\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "# 잘 섞인 전체 데이터를 8:2 비율로 훈련 데이터와 테스트 데이터로 분리\n",
        "# 분리할 테스트 데이터의 크기를 정의\n",
        "n_of_val = int(len(encoder_input)*0.2)\n",
        "print('테스트 데이터의 수 :', n_of_val)"
      ],
      "metadata": {
        "id": "4wHUc9xDgWEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "id": "RiYFvC9PgWBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정수 인코딩하기\n",
        "- 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꾸어야 한다.\n",
        "- 각 단어에 고유한 정수를 맵핑하는 작업인 단어 집합(vocabualry)을 만든다.\n",
        "<br/><br/>\n",
        "- 입력된 훈련 데이터부터 Kears의 토크나이저를 사용하여 단어 집합을 만들어보자."
      ],
      "metadata": {
        "id": "DmmmxTvbipkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 'text'\n",
        "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n"
      ],
      "metadata": {
        "id": "z0cKR8C_gV-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었다.\n",
        "- 단어 집합이 `src_tokenizer.word_index`에 저장되어 있다.\n",
        "- 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행한다.\n",
        "- 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보자."
      ],
      "metadata": {
        "id": "UeSweTRajTIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 7\n",
        "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "JMUeEC3RgV8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어의 집합 크기를 20,000으로 제한한다.\n",
        "- num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있다."
      ],
      "metadata": {
        "id": "ZW-2H25-jwF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 20000\n",
        "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 20,000으로 제한\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
      ],
      "metadata": {
        "id": "Y2ImdjClkGys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 `text_to_sequences()`를 통해 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행한다.\n",
        "- 이제 20,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않는다."
      ],
      "metadata": {
        "id": "lru1hAsKkN3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
        "\n",
        "# 잘 진행되었는지 샘플 출력\n",
        "print(encoder_input_train[:3])"
      ],
      "metadata": {
        "id": "a7k7OGBIkffB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 headlines 데이터에 대해서도 동일한 작업을 수행한다."
      ],
      "metadata": {
        "id": "jIcCyJgBkkl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'headlines'\n",
        "tar_tokenizer = Tokenizer() # 토크나이저 정의\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
        "\n",
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "oj9P8fE1kfc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 10000  # 등장 빈도가 적은 단어를 제외한 단어 집합 크기\n",
        "\n",
        "tar_tokenizer = Tokenizer(num_words=tar_vocab)  # 단어 집합 크기 설정해서 다시 생성\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
        "\n",
        "# 잘 변환되었는지 확인\n",
        "print('input')\n",
        "print('input ',decoder_input_train[:5])\n",
        "print('target')\n",
        "print('decoder ',decoder_target_train[:5])"
      ],
      "metadata": {
        "id": "EccQQANDkfaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 빈도수가 낮은 단어만으로 구성되어 있는 데이터들은 empty 데이터로 됐을 가능성이 있다.\n",
        "- 하지만, 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에 삭제되지 않는다.\n",
        "- 그래서 길이가 0이 된 'headlines'의 실제 길이는 1로 나올 것이다.\n",
        "- 길이가 1인 경우의 인덱스를 찾아 삭제해보자."
      ],
      "metadata": {
        "id": "uG_uBrfGlMBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
        "\n",
        "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
        "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
        "\n",
        "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
        "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
        "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
        "\n",
        "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
        "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
        "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "id": "F51ckbMkkfYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패딩하기\n",
        "- 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해줘야 한다.\n",
        "- 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춘다."
      ],
      "metadata": {
        "id": "cXghaVmdl0G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "KfA7Es_3kfVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seq2seq 모델 설계하기\n",
        "- hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터이다.\n",
        "- 이는 LSTM의 용량의 크기, LSTM에서의 뉴런의 개수이다.\n",
        "- 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 올라가는 것은 아니다.\n",
        "<br/><br/>\n",
        "인코더\n",
        "- LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였다. => 모델의 용량 증가\n",
        "- 3개의 층을 지나서 인코더로부터 나온 출력 벡터를 디코더로 보내준다.\n",
        "- dropout + recurrent dropout = Variational Dropout<br/>\n",
        "=> regularization을 해주는 효과 + 과적합 방지\n",
        "<br/><br/>\n",
        "디코더\n",
        "- 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일\n",
        "- 하지만, initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 한다.\n",
        "- 디코더의 출력층에서는 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 한다. => Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용 => `Dense(tar_vocab, activation='softmax')`"
      ],
      "metadata": {
        "id": "ApmOZjDimY7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# 인코더 설계 시작\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# 인코더\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "# 인코더의 임베딩 층\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "# 인코더의 LSTM 1\n",
        "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
        "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# 인코더의 LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout = 0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# 인코더의 LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout = 0.4)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
      ],
      "metadata": {
        "id": "6RXW37HpkfSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 설계\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# 디코더의 임베딩 층\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 디코더의 LSTM\n",
        "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
      ],
      "metadata": {
        "id": "bizTLeb0kfQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TXoYKDmHkfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
        "- 일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 더 나은 성능을 얻을 수 있다.\n",
        "- 실습 내용을 참고하여 어텐션 메커니즘을 사용한 seq2seq를 설계해 보세요."
      ],
      "metadata": {
        "id": "BcL-pBGDUasb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야 한다는 뜻이다."
      ],
      "metadata": {
        "id": "wrSZDo8BpIw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import AdditiveAttention\n",
        "\n",
        "# 어텐션 층(어텐션 함수)\n",
        "attn_layer = AdditiveAttention(name='attention_layer')\n",
        "\n",
        "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "\n",
        "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AJDCs8BTWMji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 훈련하기\n",
        "EarlyStopping\n",
        "- 특정 조건이 충족되면 훈련을 멈추는 역할<br/> => `es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)`\n",
        "- val_loss를 관찰하다가 손실이 줄어들지 않고 증가하는 현상이 2회(`patience = 2`) 관측되면 학습 종료.\n",
        "- epochs가 크게 설정되어 있어도 모델 훈련을 최적점에서 멈춤."
      ],
      "metadata": {
        "id": "i0AeLNDfpr6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
        "          batch_size=256, callbacks=[es], epochs=50)"
      ],
      "metadata": {
        "id": "wIC9EIvIprYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- train_loss와 val_loss의 데이터의 손실이 줄어드는 과정을 시각화하기"
      ],
      "metadata": {
        "id": "Ri9zscZ9tdAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bpnOxAgVprWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인퍼런스 모델 구현하기\n",
        "- 테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 3개의 사전을 아래와 같이 준비한다.\n",
        "<br/><br/>\n",
        "seq2seq\n",
        "- 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행\n",
        "- 훈련 단계에서는 인코더와 디코더를 묶어 모델 하나만 준비한다.\n",
        "<br/><br/>\n",
        "인퍼런스 단계\n",
        "- 정답이 없다.\n",
        "- 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작\n",
        "- 인퍼런스를 위한 모델 설계를 별도로 만든다.\n",
        "- 인코더 모델과 디코더 모델을 분리해서 설계.\n",
        "\n"
      ],
      "metadata": {
        "id": "6DsfzIU5t0_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
      ],
      "metadata": {
        "id": "KA6tTA9iprTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 설계\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
      ],
      "metadata": {
        "id": "yfh470hvprQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어텐션 메커니즘을 사용하는 출력층을 설계"
      ],
      "metadata": {
        "id": "LS-JijXhvIk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 함수\n",
        "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n",
        "\n",
        "# 최종 디코더 모델\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "EfTY-d5UprOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수"
      ],
      "metadata": {
        "id": "Ool5vL-fvMxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_token!='eostoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "xfxjwuBtprLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
        "- 원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해 보세요."
      ],
      "metadata": {
        "id": "y0tStZwaUen6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원문의 정수 시퀀스를 텍스트 시퀀스로 변환"
      ],
      "metadata": {
        "id": "s84Z1YZ-veOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2text(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if (i!=0):\n",
        "            temp = temp + src_index_to_word[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2headlines(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
        "            temp = temp + tar_index_to_word[i] + ' '\n",
        "    return temp"
      ],
      "metadata": {
        "id": "isym6_vKWMwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제 요약과 예측된 요약문을 비교해보자."
      ],
      "metadata": {
        "id": "hR9Ser9IvoHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(\"* 원문 :\", seq2text(encoder_input_test[i]))\n",
        "    print(\"* 실제 요약 :\", seq2headlines(decoder_input_test[i]))\n",
        "    print(\"* 추상적 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "-VwkVLdevyz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기존의 요약과는 다른 요약을 출력하면서도 원문의 내용을 담고 있는 의미 있는 요약들을 보인다.\n"
      ],
      "metadata": {
        "id": "PHX4s5fswEvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. Summa을 이용해서 추출적 요약해보기\n",
        "- 추상적 요약은 추출적 요약과는 달리 문장의 표현력을 다양하게 가져갈 수 있지만, 추출적 요약에 비해서 난이도가 높아요.\n",
        "- 반대로 말하면 추출적 요약은 추상적 요약에 비해 난이도가 낮고 기존 문장에서 문장을 꺼내오는 것이므로 잘못된 요약이 나올 가능성이 낮다.\n",
        "- Summa의 summarize를 사용하여 추출적 요약을 해보세요."
      ],
      "metadata": {
        "id": "xaiiz4pdUh6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "추상적 요약\n",
        "- 원문에 없던 단어를 사용해서 요약\n",
        "- 자유로운 텍스트 생성 모델(RNN, LSTM, Transformer 등)을 사용\n",
        "- 정확성과 일관성이 유지되지 않을 수 있다.\n",
        "<br/><br/>\n",
        "\n",
        "추출적 요약\n",
        "- 원문에서 중요한 핵심 문장 또는 단어를 뽑아 구성된 요약문\n",
        "- 언어 표현 능력이 제한되어 문장이 매끄럽지 않다.\n",
        "- 대표적인 알고리즘 TextRank\n",
        "- 요약에 포함될 문장을 선택하기 위해 문장 중요도를 평가하는 알고리즘(TF-IDF, 문장 중요도 그래프)을 사용."
      ],
      "metadata": {
        "id": "397cOxC3xhyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Summa의 suimmarize()의 인자로 사용되는 값\n",
        "> text (str) - 요약할 텍스트.<br/>\n",
        "> ratio (float, optional) - 요약문에서 원본에서 선택되는 문장 비율, 0~1 사이값<br/>\n",
        "> words (int or None, optional) - 출력에 포함할 단어 수.<br/>\n",
        "> 만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시한다.<br/>\n",
        "> split (bool, optional) - True면 문장 list / False는 조인(join)된 문자열을 반환"
      ],
      "metadata": {
        "id": "OYtPV3AUxrkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "from summa.summarizer import summarize\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
        "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
        "print(data[:1500])"
      ],
      "metadata": {
        "id": "7v0ZIUd_UOrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(\"* 원문 :\", data['text'][i])\n",
        "    print(\"* 실제 요약 :\", data['headlines'][i])\n",
        "    print(\"* 추출적 요약 :\", summarize(data['text'][i], ratio=0.5))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "PEBI_qZwzXAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래는 추상적 요약(Abstractive Summarization)을 나타낸 표이다.\n",
        "- 실제 요약과는 다른 내용으로 요약을 해내는 경우가 꽤 있다."
      ],
      "metadata": {
        "id": "o78qwXcI4imB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|headlines|추상적 요약|\n",
        "|:---:|:---:|\n",
        "|ramdev patanjali products fail uttarakhand quality test|patanjali announces scholarship for iit india|\n",
        "|rajasthan bjp sitting mlas in st candidate list|bjp releases first list of candidates for bjp assembly|\n",
        "|congress naxals as up cm yogi|bjp mla calls bjp workers in uttar pradesh|\n",
        "|human foetus found near garbage dumping site in delhi|month old found found in delhi hospital|\n",
        "|indian agency seeks us help in probing indigo engine smoke incident|killed in jet jet crashes off plane crashes in iran|"
      ],
      "metadata": {
        "id": "LnS4FyGq1pNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래는 추출적 요약(Extractive Summarization)을 나타낸 표이다.\n",
        "- 요약을 했지만 문장이 길다."
      ],
      "metadata": {
        "id": "DZIRgsPy4ucP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|headlines|추출적 요약|\n",
        "|:---:|:---:|\n",
        "|upGrad learner switches to career in ML & Al with 90% salary hike|upGrad's Online Power Learning has powered 3 lakh+ careers.|\n",
        "|Delhi techie wins free food from Swiggy for one year on CRED|Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.|\n",
        "|New Zealand end Rohit Sharma-led India's 12-match winning streak|The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.|\n",
        "|Aegon life iTerm insurance plan helps customers save tax|Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.|\n",
        "|Have known Hirani for yrs, what if MeToo claims are not true: Sonam|Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman.|"
      ],
      "metadata": {
        "id": "5OhOlN-G3xi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 요약적으로 따지자면 추출적 요약이 더 요약을 잘 한것같다."
      ],
      "metadata": {
        "id": "fQey2w8b5Flg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "- 추상적 요약은 전처리를 하고 했지만 요약을 해보면 생각보다 원문과는 다른 내용을 보이고 있다. 다른 방식으로 전처리나 모델링을 해서 더욱 정확한 추상적 요약을 해보고싶다.\n",
        "- 추출적 요약이 원문을 그대로 가져오다보니 더 요약을 잘하는 것같다."
      ],
      "metadata": {
        "id": "f1OXvfsx5Kxq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjQ79VKPzzaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}