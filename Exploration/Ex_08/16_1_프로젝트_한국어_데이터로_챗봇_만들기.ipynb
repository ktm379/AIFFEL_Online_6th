{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "1ZC-x8KKPODH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcvfuPxLQDaM"
   },
   "source": [
    "# Step 1. 데이터 수집하기\n",
    "- 한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터 : [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n",
    "    1. 챗봇 트레이닝용 문답 페어 11,876개\n",
    "    2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "7ExFGNDiQF4g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "clNy3qdzVLwy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    }
   ],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = len(data)\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jDjwpD3-VLuq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         12시 땡!\n",
       "1                    1지망 학교 떨어졌어\n",
       "2                   3박4일 놀러가고 싶다\n",
       "3                3박4일 정도 놀러가고 싶다\n",
       "4                        PPL 심하네\n",
       "                  ...           \n",
       "11818             훔쳐보는 것도 눈치 보임.\n",
       "11819             훔쳐보는 것도 눈치 보임.\n",
       "11820                흑기사 해주는 짝남.\n",
       "11821    힘든 연애 좋은 연애라는게 무슨 차이일까?\n",
       "11822                 힘들어서 결혼할까봐\n",
       "Name: Q, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "l8PqYZ1qVLsH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      하루가 또 가네요.\n",
       "1                       위로해 드립니다.\n",
       "2                     여행은 언제나 좋죠.\n",
       "3                     여행은 언제나 좋죠.\n",
       "4                      눈살이 찌푸려지죠.\n",
       "                   ...           \n",
       "11818          티가 나니까 눈치가 보이는 거죠!\n",
       "11819               훔쳐보는 거 티나나봐요.\n",
       "11820                      설렜겠어요.\n",
       "11821    잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822          도피성 결혼은 하지 않길 바라요.\n",
       "Name: A, Length: 11823, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k4x4xr0PwmI"
   },
   "source": [
    "# Step 2. 데이터 전처리하기\n",
    "- 영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리르 수행해야 할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKcjoeLmYN6I"
   },
   "source": [
    "- 정규 표현식을 사용하여 구두점(punctuation)을 제거하여 단어를 토크나이징(tokenizing)하는 일에 방해가 되지 않도록 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "M9CjKWh4Pdj7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "  sentence = sentence.lower()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "KWbNIfC8bHeV"
   },
   "outputs": [],
   "source": [
    "def load_conversations():\n",
    "    questions, answers = [], []\n",
    "    for i in range(MAX_SAMPLES):\n",
    "        questions.append(preprocess_sentence(data['Q'].values[i]))\n",
    "        answers.append(preprocess_sentence(data['A'].values[i]))\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8Jx9gOZ4cd2w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "jwq7iWHXdWHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opf2E7nXQPMV"
   },
   "source": [
    "# Step 3. SubwordTextEncoder 사용하기\n",
    "- 한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmXGeNORewqZ"
   },
   "source": [
    "1. Tensorflow datasets SubwordTextEncoder를 토크나이저 한다. 단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고, 각 토큰을 고유한 정수로 인코딩한다.\n",
    "2. 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가한다.\n",
    "3. 최대 길이 MAX_LENGHT인 40을 넘는 문장들은 필터링한다.\n",
    "4. MAX_LENGTH보다 길이가 짧은 문장들은 40에 맞도록 패딩한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlhbFtzRfJ91"
   },
   "source": [
    "### 1. 단어장(Vocablulary) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "UpUh_saOQPdU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFmcm9prfbaz"
   },
   "source": [
    "- 시작 토큰과 종료 토큰을 임의로 단어장에 추가하여 정수를 부여\n",
    "- 이미 생성된 단어장의 번홍와 겹치지 않도록 1이 큰 수를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "PSOqp9IlfTKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8127]\n",
      "END_TOKEN의 번호 : [8128]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "hGy_yidufTFQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8129\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWoLVitmfttf"
   },
   "source": [
    "### 2. 각 단어를 고유한 정수로 인코딩 & 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LplUrKALgCxr"
   },
   "source": [
    "- 위에서 tokenizer를 정의하고 단어장을 만들었다면,\n",
    "- `tokenizer.encode()`로 각 단어를 정수로 변환\n",
    "- 또는 `tokenizer.decode()`로 정수 시퀀스로 단어로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "paLCsYJAfS_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "93GjrGjbfS9W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNTXTpt0gWBX"
   },
   "source": [
    "- 40을 넘는 경우 필터링\n",
    "- 필터링을 제외한 샘플의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "zyrG4jRQfS6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8129\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5KW1VNngeW0"
   },
   "source": [
    "### 3. 교사 강요(Teacher Forcing) 사용하기\n",
    "- 질문과 답변의 쌍을 tf.data.Dataset API의 입력으로 사용하여 파이프라인 구성\n",
    "- 교사 강요를 위해서 answers[:, : -1]를 디코더의 입력값,\n",
    "- answers[:, 1:]를 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "G_54V881fS4D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Laypt4fHQqSa"
   },
   "source": [
    "# Step 4. 모델 구성하기\n",
    "- 위 실습 내용을 참고하여 트랜스포머 모델을 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfm1F9bFi2x3"
   },
   "source": [
    "#### 포지셔널 인코딩\n",
    "- 임베딩 행렬과 포지셔널 행렬이라는 두 행렬을 더함으로써 각 단어 벡터에 위치 정보를 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "JytHCMY4hSbD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0])\n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvRYDepki82s"
   },
   "source": [
    "#### 스케일드 닷 프로덕트 어텐션\n",
    "1. 내적(dot product)을 통해 단어 벡터 간 유사도를 구함\n",
    "2. 특정 값을 분모로 나눠주는 방식을 Q와 K의 유사도를 구함\n",
    "- 이를 스케일드 닷 프로덕트 어텐션(Scaled Dot Product Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "TQPSnCdMhSYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO43p1lQi_cW"
   },
   "source": [
    "#### 멀티 헤드 어텐션\n",
    "- 병렬적으로 몇 개의 어텐션 연산을 수행할지를 결정하는 하이퍼파라미터\n",
    "- 각각 다른 관점에서 어텐션을 수행하므로 한 번의 어텐션만 수행했다면 놓칠 수도 있던 정보를 캐치\n",
    "- 내부적으로는 스케일드 닷 프로덕트 어텐션 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "jc9ONVsGhSV8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)  # query에 Dense 레이어를 적용합니다\n",
    "    key = self.key_dense(key)  # key에 Dense 레이어를 적용합니다\n",
    "    value = self.value_dense(value)  # value에 Dense 레이어를 적용합니다\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQymuwjDm5"
   },
   "source": [
    "#### 마스킹\n",
    "- 특정 값들을 가려서 실제 연산에 방해가 되지 않도록 하는 기법\n",
    "- 트랜스포머에서는 어텐션을 위해서 크게 패딩 마스킹(Padding Masking)과 룩 어헤드 마스킹(Look-ahead Masking)을 사용\n",
    "- 패딩 마스킹는 패딩 토큰을 이용한 방법<br/>\n",
    "=> 실제 의미가 있는 단어가 아닌 0은 제외하는 방식<br/>\n",
    "=> 이를 위해 숫자 0인 위치를 체크\n",
    "<br/><br/>\n",
    "- 룩 어헤드 마스킹은 다음에 나올 단어를 참고하지 않도록 가리는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "yb_GV2B7hSTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 패딩 마스킹\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "W57E90RjhSQo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 룩 어헤드 마스킹\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se3QBcAfklvw"
   },
   "source": [
    "#### 인코더\n",
    "- 총 2개의 서브 층으로 이루진다.\n",
    "    1. 셀프 어텐션\n",
    "        - 멀티 헤드 어텐션으로 병렬적으로 이루어\n",
    "    2. 피드 포워드 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "FiDH7Pe6hSN5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPu5vUznm7s3"
   },
   "source": [
    "#### 인코더 층을 쌓아 인코더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "aq2xzl1mhSLR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8GDiyRznM4W"
   },
   "source": [
    "#### 디코더\n",
    "- 총 3개의 서브 층으로 이루어져있다.\n",
    "    1. 셀프 어텐션\n",
    "    2. 인코더-디코더 어텐션\n",
    "    3. 피드 포워드 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "YAnedhaGnJW-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBFNAXKxnya1"
   },
   "source": [
    "#### 디코더 층을 쌓아 디코더 만들기\n",
    "- 임베딩 층과 포지셔널 인코딩을 연결\n",
    "- 사용자가 원하는 만큼 디코더 층을 쌓아 트랜스포머 디코더 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "2tZgSUdEnJUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XctqlqPdoFEW"
   },
   "source": [
    "#### 트랜스포머 모델 정의\n",
    "- 인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1nh0Az8GQuHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3viOk3goN8n"
   },
   "source": [
    "#### 트랜스포머 모델 생성\n",
    "- `num_layers`, `d_model`, `units`는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "cs7JyygShHJv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3135232     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3662592     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8129)   2089153     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,886,977\n",
      "Trainable params: 8,886,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZXU1NXeoamQ"
   },
   "source": [
    "#### 손실 함수\n",
    "- 레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스킹을 적용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "RxfZbjMyhHHv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWWwncuIouOM"
   },
   "source": [
    "#### 커스텀 된 학습률\n",
    "- 커스텀 학습률 스케줄링(Custom Learing rate Scheduling)\n",
    "    - 학습 초기에 lr을 급격히 높였다가,\n",
    "    - 이후 trian step이 진행됨에 따라 서서히 낮추어가며 안정적으로 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "GLTA8sm0hHFC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3WgII37pEUb"
   },
   "source": [
    "- 커스텀 학습률 스케줄링 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "G9Snbu5YhHCb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "OZpXJkOZhG_4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "1r8LeJgXhG9O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "185/185 [==============================] - 15s 54ms/step - loss: 1.4483 - accuracy: 0.0272\n",
      "Epoch 2/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.1796 - accuracy: 0.0493\n",
      "Epoch 3/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.0037 - accuracy: 0.0505\n",
      "Epoch 4/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.9247 - accuracy: 0.0545\n",
      "Epoch 5/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.8658 - accuracy: 0.0577\n",
      "Epoch 6/10\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.8060 - accuracy: 0.0619\n",
      "Epoch 7/10\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.7398 - accuracy: 0.0681\n",
      "Epoch 8/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.6667 - accuracy: 0.0759\n",
      "Epoch 9/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.5876 - accuracy: 0.0845\n",
      "Epoch 10/10\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.5058 - accuracy: 0.0940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa842c7aa00>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqFKKeQ4Quez"
   },
   "source": [
    "# Step 5. 모델 평가하기\n",
    "- Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "LyHtXGE3Q0e_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임의의 입력 문장에 대하여 decoder_inference() 함수를 호출하여 챗봇의 대답을 얻는 sentence_generation() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "    \n",
    "    return predicted_sentence\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\n",
    "    \"오늘 뭐 먹을까요?\",\n",
    "    \"자살하고 싶어\",\n",
    "    \"오늘의 날씨는 어때요?\",\n",
    "    \"넌 누구야?\",\n",
    "    \"인생은 뭘까\",\n",
    "    \"너 뭐 좋아해?\",\n",
    "    \"공부 좀 대신해줘\",\n",
    "    \"사랑해\",\n",
    "    \"피곤해\",\n",
    "    \"그동안 즐거웠어\",\n",
    "    \"자니?\",\n",
    "    \"힘들어\",\n",
    "    \"놀아줘\",\n",
    "    \"종교가 뭐야?\",\n",
    "    \"저녁 메뉴 추천해줘\",\n",
    "    \"오랜만이야\",\n",
    "    \"너는 어떻게 기분전환해?\",\n",
    "    \"너의 꿈이 뭐야?\",\n",
    "    \"너는 행복해?\",\n",
    "    \"웃어줘\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers(question_list):\n",
    "    for question in question_list:\n",
    "        sentence_generation(question)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs = 10 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐 먹을까요?\n",
      "출력 : 잘 찾아보세요 .\n",
      "\n",
      "입력 : 자살하고 싶어\n",
      "출력 : 잘 찾아보세요 .\n",
      "\n",
      "입력 : 오늘의 날씨는 어때요?\n",
      "출력 : 그게 최고죠 .\n",
      "\n",
      "입력 : 넌 누구야?\n",
      "출력 : 다른 곳에 쓰려고 운을 아껴뒀나봐요 .\n",
      "\n",
      "입력 : 인생은 뭘까\n",
      "출력 : 저도 좋아해요 .\n",
      "\n",
      "입력 : 너 뭐 좋아해?\n",
      "출력 : 저는 위로해드리는 로봇이에요 .\n",
      "\n",
      "입력 : 공부 좀 대신해줘\n",
      "출력 : 지금도 늦지 않았어요 .\n",
      "\n",
      "입력 : 사랑해\n",
      "출력 : 조금만 드세요 .\n",
      "\n",
      "입력 : 피곤해\n",
      "출력 : 곧 방학이예요 .\n",
      "\n",
      "입력 : 그동안 즐거웠어\n",
      "출력 : 감기 조심하세요 .\n",
      "\n",
      "입력 : 자니?\n",
      "출력 : 맛있게 드세요 .\n",
      "\n",
      "입력 : 힘들어\n",
      "출력 : 조금만 더 버텨보세요 .\n",
      "\n",
      "입력 : 놀아줘\n",
      "출력 : 같이 놀아요 .\n",
      "\n",
      "입력 : 종교가 뭐야?\n",
      "출력 : 화장실 가세요 .\n",
      "\n",
      "입력 : 저녁 메뉴 추천해줘\n",
      "출력 : 맛있는 거 드세요 .\n",
      "\n",
      "입력 : 오랜만이야\n",
      "출력 : 더 좋은 사람 만날 수 있을 거예요 .\n",
      "\n",
      "입력 : 너는 어떻게 기분전환해?\n",
      "출력 : 다른 곳에 쓰려고 운을 아껴뒀나봐요 .\n",
      "\n",
      "입력 : 너의 꿈이 뭐야?\n",
      "출력 : 저는 위로해드리는 로봇이에요 .\n",
      "\n",
      "입력 : 너는 행복해?\n",
      "출력 : 잘 찾아보세요 .\n",
      "\n",
      "입력 : 웃어줘\n",
      "출력 : 오늘 일찍 주무세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs = 300 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.4235 - accuracy: 0.1039\n",
      "Epoch 2/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.3435 - accuracy: 0.1151\n",
      "Epoch 3/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.2697 - accuracy: 0.1260\n",
      "Epoch 4/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.2058 - accuracy: 0.1359\n",
      "Epoch 5/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1519 - accuracy: 0.1450\n",
      "Epoch 6/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1092 - accuracy: 0.1532\n",
      "Epoch 7/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0805 - accuracy: 0.1582\n",
      "Epoch 8/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0626 - accuracy: 0.1613\n",
      "Epoch 9/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0517 - accuracy: 0.1635\n",
      "Epoch 10/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0457 - accuracy: 0.1644\n",
      "Epoch 11/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0433 - accuracy: 0.1646\n",
      "Epoch 12/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0416 - accuracy: 0.1647\n",
      "Epoch 13/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0374 - accuracy: 0.1658\n",
      "Epoch 14/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0331 - accuracy: 0.1668\n",
      "Epoch 15/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0290 - accuracy: 0.1677\n",
      "Epoch 16/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0254 - accuracy: 0.1685\n",
      "Epoch 17/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0227 - accuracy: 0.1694\n",
      "Epoch 18/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0209 - accuracy: 0.1697\n",
      "Epoch 19/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0194 - accuracy: 0.1701\n",
      "Epoch 20/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0177 - accuracy: 0.1706\n",
      "Epoch 21/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0164 - accuracy: 0.1709\n",
      "Epoch 22/300\n",
      "185/185 [==============================] - 10s 56ms/step - loss: 0.0143 - accuracy: 0.1714\n",
      "Epoch 23/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0141 - accuracy: 0.1716\n",
      "Epoch 24/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0127 - accuracy: 0.1719\n",
      "Epoch 25/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0119 - accuracy: 0.1721\n",
      "Epoch 26/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0115 - accuracy: 0.1722\n",
      "Epoch 27/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0108 - accuracy: 0.1723\n",
      "Epoch 28/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0102 - accuracy: 0.1725\n",
      "Epoch 29/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0091 - accuracy: 0.1728\n",
      "Epoch 30/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0090 - accuracy: 0.1728\n",
      "Epoch 31/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0089 - accuracy: 0.1728\n",
      "Epoch 32/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0084 - accuracy: 0.1731\n",
      "Epoch 33/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0075 - accuracy: 0.1732\n",
      "Epoch 34/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0074 - accuracy: 0.1732\n",
      "Epoch 35/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0075 - accuracy: 0.1732\n",
      "Epoch 36/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0070 - accuracy: 0.1732\n",
      "Epoch 37/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0068 - accuracy: 0.1733\n",
      "Epoch 38/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0066 - accuracy: 0.1733\n",
      "Epoch 39/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0061 - accuracy: 0.1735\n",
      "Epoch 40/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0064 - accuracy: 0.1734\n",
      "Epoch 41/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0058 - accuracy: 0.1735\n",
      "Epoch 42/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0057 - accuracy: 0.1736\n",
      "Epoch 43/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0055 - accuracy: 0.1736\n",
      "Epoch 44/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0053 - accuracy: 0.1737\n",
      "Epoch 45/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0055 - accuracy: 0.1736\n",
      "Epoch 46/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0048 - accuracy: 0.1738\n",
      "Epoch 47/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0046 - accuracy: 0.1738\n",
      "Epoch 48/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0049 - accuracy: 0.1738\n",
      "Epoch 49/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0047 - accuracy: 0.1738\n",
      "Epoch 50/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0046 - accuracy: 0.1738\n",
      "Epoch 51/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0041 - accuracy: 0.1738\n",
      "Epoch 52/300\n",
      "185/185 [==============================] - 10s 56ms/step - loss: 0.0043 - accuracy: 0.1739\n",
      "Epoch 53/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0042 - accuracy: 0.1738\n",
      "Epoch 54/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0043 - accuracy: 0.1739\n",
      "Epoch 55/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0038 - accuracy: 0.1739\n",
      "Epoch 56/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0041 - accuracy: 0.1739\n",
      "Epoch 57/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0039 - accuracy: 0.1740\n",
      "Epoch 58/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0037 - accuracy: 0.1740\n",
      "Epoch 59/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0036 - accuracy: 0.1740\n",
      "Epoch 60/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0035 - accuracy: 0.1740\n",
      "Epoch 61/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0034 - accuracy: 0.1740\n",
      "Epoch 62/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0033 - accuracy: 0.1741\n",
      "Epoch 63/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0032 - accuracy: 0.1740\n",
      "Epoch 64/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0032 - accuracy: 0.1740\n",
      "Epoch 65/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0032 - accuracy: 0.1740\n",
      "Epoch 66/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0029 - accuracy: 0.1741\n",
      "Epoch 67/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0030 - accuracy: 0.1741\n",
      "Epoch 68/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0030 - accuracy: 0.1741\n",
      "Epoch 69/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0027 - accuracy: 0.1741\n",
      "Epoch 70/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0029 - accuracy: 0.1741\n",
      "Epoch 71/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0027 - accuracy: 0.1741\n",
      "Epoch 72/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0026 - accuracy: 0.1741\n",
      "Epoch 73/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0027 - accuracy: 0.1741\n",
      "Epoch 74/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0026 - accuracy: 0.1741\n",
      "Epoch 75/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0026 - accuracy: 0.1742\n",
      "Epoch 76/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0026 - accuracy: 0.1741\n",
      "Epoch 77/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0025 - accuracy: 0.1741\n",
      "Epoch 78/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0024 - accuracy: 0.1742\n",
      "Epoch 79/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0023 - accuracy: 0.1742\n",
      "Epoch 80/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0024 - accuracy: 0.1741\n",
      "Epoch 81/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0023 - accuracy: 0.1742\n",
      "Epoch 82/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0022 - accuracy: 0.1741\n",
      "Epoch 83/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0021 - accuracy: 0.1742\n",
      "Epoch 84/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1742\n",
      "Epoch 85/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1742\n",
      "Epoch 86/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1742\n",
      "Epoch 87/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0022 - accuracy: 0.1742\n",
      "Epoch 88/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0021 - accuracy: 0.1742\n",
      "Epoch 89/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1742\n",
      "Epoch 90/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 91/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1742\n",
      "Epoch 92/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1742\n",
      "Epoch 93/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1742\n",
      "Epoch 94/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1742\n",
      "Epoch 95/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1743\n",
      "Epoch 96/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0020 - accuracy: 0.1742\n",
      "Epoch 97/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1743\n",
      "Epoch 98/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 99/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 100/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 101/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 102/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0019 - accuracy: 0.1742\n",
      "Epoch 103/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 104/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 105/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0016 - accuracy: 0.1743\n",
      "Epoch 106/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 107/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0018 - accuracy: 0.1742\n",
      "Epoch 108/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1743\n",
      "Epoch 109/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 110/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0016 - accuracy: 0.1743\n",
      "Epoch 111/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 112/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0016 - accuracy: 0.1743\n",
      "Epoch 113/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0015 - accuracy: 0.1743\n",
      "Epoch 114/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 115/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0017 - accuracy: 0.1742\n",
      "Epoch 116/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1743\n",
      "Epoch 117/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 118/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 119/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 120/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 121/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 122/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1743\n",
      "Epoch 123/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 124/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 125/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 126/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 127/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 128/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 129/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 130/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 131/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 132/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 133/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 134/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 135/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 136/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 137/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 138/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 139/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 140/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0015 - accuracy: 0.1743\n",
      "Epoch 141/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 142/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 143/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 144/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 145/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0014 - accuracy: 0.1743\n",
      "Epoch 146/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 147/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 148/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 149/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 150/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 151/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 152/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 153/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 154/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 155/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 156/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 157/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 158/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 159/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 160/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 161/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 162/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 163/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 164/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0013 - accuracy: 0.1743\n",
      "Epoch 165/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 166/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 167/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 168/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 169/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 170/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 171/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 172/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 173/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 174/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 175/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 176/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 177/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 178/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.5914e-04 - accuracy: 0.1743\n",
      "Epoch 179/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 180/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 181/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 182/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.9609e-04 - accuracy: 0.1743\n",
      "Epoch 183/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 184/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 185/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 186/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1744\n",
      "Epoch 187/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 188/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 189/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 190/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 191/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 192/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 193/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1744\n",
      "Epoch 194/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7468e-04 - accuracy: 0.1744\n",
      "Epoch 195/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 196/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 197/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 198/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4279e-04 - accuracy: 0.1743\n",
      "Epoch 199/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 200/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.9967e-04 - accuracy: 0.1744\n",
      "Epoch 201/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.6452e-04 - accuracy: 0.1743\n",
      "Epoch 202/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.9845e-04 - accuracy: 0.1743\n",
      "Epoch 203/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.5206e-04 - accuracy: 0.1743\n",
      "Epoch 204/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 205/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 206/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 207/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.6765e-04 - accuracy: 0.1743\n",
      "Epoch 208/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3686e-04 - accuracy: 0.1744\n",
      "Epoch 209/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2356e-04 - accuracy: 0.1743\n",
      "Epoch 210/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7806e-04 - accuracy: 0.1743\n",
      "Epoch 211/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 212/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 213/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0196e-04 - accuracy: 0.1743\n",
      "Epoch 214/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2414e-04 - accuracy: 0.1744\n",
      "Epoch 215/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 216/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 217/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5477e-04 - accuracy: 0.1744\n",
      "Epoch 218/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.3096e-04 - accuracy: 0.1744\n",
      "Epoch 219/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9997e-04 - accuracy: 0.1744\n",
      "Epoch 220/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 221/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0012 - accuracy: 0.1743\n",
      "Epoch 222/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0365e-04 - accuracy: 0.1744\n",
      "Epoch 223/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.3480e-04 - accuracy: 0.1744\n",
      "Epoch 224/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4625e-04 - accuracy: 0.1744\n",
      "Epoch 225/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6410e-04 - accuracy: 0.1743\n",
      "Epoch 226/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4909e-04 - accuracy: 0.1743\n",
      "Epoch 227/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0576e-04 - accuracy: 0.1743\n",
      "Epoch 228/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7969e-04 - accuracy: 0.1743\n",
      "Epoch 229/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0011 - accuracy: 0.1744\n",
      "Epoch 230/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2501e-04 - accuracy: 0.1744\n",
      "Epoch 231/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7979e-04 - accuracy: 0.1743\n",
      "Epoch 232/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4682e-04 - accuracy: 0.1743\n",
      "Epoch 233/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4532e-04 - accuracy: 0.1743\n",
      "Epoch 234/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9359e-04 - accuracy: 0.1744\n",
      "Epoch 235/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.2685e-04 - accuracy: 0.1743\n",
      "Epoch 236/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4763e-04 - accuracy: 0.1743\n",
      "Epoch 237/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.4991e-04 - accuracy: 0.1744\n",
      "Epoch 238/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7268e-04 - accuracy: 0.1744\n",
      "Epoch 239/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5107e-04 - accuracy: 0.1743\n",
      "Epoch 240/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.7749e-04 - accuracy: 0.1743\n",
      "Epoch 241/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.1859e-04 - accuracy: 0.1744\n",
      "Epoch 242/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7036e-04 - accuracy: 0.1744\n",
      "Epoch 243/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9262e-04 - accuracy: 0.1743\n",
      "Epoch 244/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5798e-04 - accuracy: 0.1744\n",
      "Epoch 245/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0011 - accuracy: 0.1743\n",
      "Epoch 246/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.9897e-04 - accuracy: 0.1743\n",
      "Epoch 247/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.4246e-04 - accuracy: 0.1744\n",
      "Epoch 248/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.1045e-04 - accuracy: 0.1744\n",
      "Epoch 249/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 9.7061e-04 - accuracy: 0.1743\n",
      "Epoch 250/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 9.7740e-04 - accuracy: 0.1743\n",
      "Epoch 251/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7501e-04 - accuracy: 0.1743\n",
      "Epoch 252/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 9.2355e-04 - accuracy: 0.1744\n",
      "Epoch 253/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 9.3698e-04 - accuracy: 0.1744\n",
      "Epoch 254/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4356e-04 - accuracy: 0.1743\n",
      "Epoch 255/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4858e-04 - accuracy: 0.1743\n",
      "Epoch 256/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8740e-04 - accuracy: 0.1744\n",
      "Epoch 257/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4923e-04 - accuracy: 0.1744\n",
      "Epoch 258/300\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 9.7824e-04 - accuracy: 0.1743\n",
      "Epoch 259/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6119e-04 - accuracy: 0.1743\n",
      "Epoch 260/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.8759e-04 - accuracy: 0.1743\n",
      "Epoch 261/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5989e-04 - accuracy: 0.1744\n",
      "Epoch 262/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3077e-04 - accuracy: 0.1744\n",
      "Epoch 263/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9115e-04 - accuracy: 0.1743\n",
      "Epoch 264/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7752e-04 - accuracy: 0.1743\n",
      "Epoch 265/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7084e-04 - accuracy: 0.1744\n",
      "Epoch 266/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7827e-04 - accuracy: 0.1743\n",
      "Epoch 267/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.1156e-04 - accuracy: 0.1743\n",
      "Epoch 268/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0010 - accuracy: 0.1743\n",
      "Epoch 269/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7087e-04 - accuracy: 0.1744\n",
      "Epoch 270/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9421e-04 - accuracy: 0.1744\n",
      "Epoch 271/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.3561e-04 - accuracy: 0.1744\n",
      "Epoch 272/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0348e-04 - accuracy: 0.1744\n",
      "Epoch 273/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5747e-04 - accuracy: 0.1744\n",
      "Epoch 274/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.8542e-04 - accuracy: 0.1743\n",
      "Epoch 275/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0664e-04 - accuracy: 0.1743\n",
      "Epoch 276/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6398e-04 - accuracy: 0.1744\n",
      "Epoch 277/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.4092e-04 - accuracy: 0.1744\n",
      "Epoch 278/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6176e-04 - accuracy: 0.1743\n",
      "Epoch 279/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3429e-04 - accuracy: 0.1744\n",
      "Epoch 280/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0127e-04 - accuracy: 0.1743\n",
      "Epoch 281/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3061e-04 - accuracy: 0.1744\n",
      "Epoch 282/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3880e-04 - accuracy: 0.1743\n",
      "Epoch 283/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9766e-04 - accuracy: 0.1743\n",
      "Epoch 284/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0968e-04 - accuracy: 0.1743\n",
      "Epoch 285/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7964e-04 - accuracy: 0.1744\n",
      "Epoch 286/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4960e-04 - accuracy: 0.1744\n",
      "Epoch 287/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.6018e-04 - accuracy: 0.1743\n",
      "Epoch 288/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7327e-04 - accuracy: 0.1743\n",
      "Epoch 289/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.0264e-04 - accuracy: 0.1744\n",
      "Epoch 290/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.9368e-04 - accuracy: 0.1743\n",
      "Epoch 291/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5923e-04 - accuracy: 0.1744\n",
      "Epoch 292/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8874e-04 - accuracy: 0.1744\n",
      "Epoch 293/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4975e-04 - accuracy: 0.1744\n",
      "Epoch 294/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8609e-04 - accuracy: 0.1744\n",
      "Epoch 295/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5044e-04 - accuracy: 0.1744\n",
      "Epoch 296/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3584e-04 - accuracy: 0.1743\n",
      "Epoch 297/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1453e-04 - accuracy: 0.1744\n",
      "Epoch 298/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0720e-04 - accuracy: 0.1744\n",
      "Epoch 299/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 9.5890e-04 - accuracy: 0.1743\n",
      "Epoch 300/300\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0652e-04 - accuracy: 0.1743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8041ffbb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐 먹을까요?\n",
      "출력 : 좀 먹어도 괜찮아요 .\n",
      "\n",
      "입력 : 자살하고 싶어\n",
      "출력 : 기다리고 있었어요 .\n",
      "\n",
      "입력 : 오늘의 날씨는 어때요?\n",
      "출력 : 딱 잘 만났네요 .\n",
      "\n",
      "입력 : 넌 누구야?\n",
      "출력 : 저는 위로봇입니다 .\n",
      "\n",
      "입력 : 인생은 뭘까\n",
      "출력 : 꽃길만 걷길 바랍니다 .\n",
      "\n",
      "입력 : 너 뭐 좋아해?\n",
      "출력 : 고백하세요 .\n",
      "\n",
      "입력 : 공부 좀 대신해줘\n",
      "출력 : 지금도 늦지 않았어요 .\n",
      "\n",
      "입력 : 사랑해\n",
      "출력 : 하늘 만큼 땅 만큼 사랑해요 .\n",
      "\n",
      "입력 : 피곤해\n",
      "출력 : 요즘 바쁜가봐요 .\n",
      "\n",
      "입력 : 그동안 즐거웠어\n",
      "출력 : 할 일이 많은데 안하는 것이요 .\n",
      "\n",
      "입력 : 자니?\n",
      "출력 : 기다리고 있었어요 .\n",
      "\n",
      "입력 : 힘들어\n",
      "출력 : 지금은 힘들겠지만 조금만 더 견뎌봐요 .\n",
      "\n",
      "입력 : 놀아줘\n",
      "출력 : 지금 그러고 있어요 .\n",
      "\n",
      "입력 : 종교가 뭐야?\n",
      "출력 : 종교가 큰 문제가 되기도 하죠 .\n",
      "\n",
      "입력 : 저녁 메뉴 추천해줘\n",
      "출력 : 냉장고 파먹기 해보세요 .\n",
      "\n",
      "입력 : 오랜만이야\n",
      "출력 : 오랜만이에요 .\n",
      "\n",
      "입력 : 너는 어떻게 기분전환해?\n",
      "출력 : 아직 안 자요 .\n",
      "\n",
      "입력 : 너의 꿈이 뭐야?\n",
      "출력 : 천천히 지워질 거예요 .\n",
      "\n",
      "입력 : 너는 행복해?\n",
      "출력 : 저는 둘이 가는 게 좋아요 .\n",
      "\n",
      "입력 : 웃어줘\n",
      "출력 : 기대를 조금씩 버려보세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs = 500 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7225e-04 - accuracy: 0.1744\n",
      "Epoch 2/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6448e-04 - accuracy: 0.1744\n",
      "Epoch 3/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.2344e-04 - accuracy: 0.1744\n",
      "Epoch 4/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.5226e-04 - accuracy: 0.1744\n",
      "Epoch 5/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.3913e-04 - accuracy: 0.1743\n",
      "Epoch 6/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6703e-04 - accuracy: 0.1744\n",
      "Epoch 7/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3604e-04 - accuracy: 0.1744\n",
      "Epoch 8/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5545e-04 - accuracy: 0.1744\n",
      "Epoch 9/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4816e-04 - accuracy: 0.1743\n",
      "Epoch 10/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1800e-04 - accuracy: 0.1743\n",
      "Epoch 11/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1111e-04 - accuracy: 0.1744\n",
      "Epoch 12/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3347e-04 - accuracy: 0.1744\n",
      "Epoch 13/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0655e-04 - accuracy: 0.1743\n",
      "Epoch 14/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3867e-04 - accuracy: 0.1744\n",
      "Epoch 15/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2302e-04 - accuracy: 0.1744\n",
      "Epoch 16/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1822e-04 - accuracy: 0.1743\n",
      "Epoch 17/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.8453e-04 - accuracy: 0.1744\n",
      "Epoch 18/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4315e-04 - accuracy: 0.1744\n",
      "Epoch 19/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7715e-04 - accuracy: 0.1743\n",
      "Epoch 20/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4064e-04 - accuracy: 0.1744\n",
      "Epoch 21/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3530e-04 - accuracy: 0.1744\n",
      "Epoch 22/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1528e-04 - accuracy: 0.1744\n",
      "Epoch 23/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7714e-04 - accuracy: 0.1743\n",
      "Epoch 24/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6449e-04 - accuracy: 0.1744\n",
      "Epoch 25/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7003e-04 - accuracy: 0.1743\n",
      "Epoch 26/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5941e-04 - accuracy: 0.1744\n",
      "Epoch 27/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5485e-04 - accuracy: 0.1744\n",
      "Epoch 28/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2598e-04 - accuracy: 0.1743\n",
      "Epoch 29/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 8.2225e-04 - accuracy: 0.1744\n",
      "Epoch 30/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9074e-04 - accuracy: 0.1744\n",
      "Epoch 31/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1706e-04 - accuracy: 0.1744\n",
      "Epoch 32/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2059e-04 - accuracy: 0.1743\n",
      "Epoch 33/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.7748e-04 - accuracy: 0.1743\n",
      "Epoch 34/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4449e-04 - accuracy: 0.1744\n",
      "Epoch 35/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3746e-04 - accuracy: 0.1744\n",
      "Epoch 36/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6099e-04 - accuracy: 0.1744\n",
      "Epoch 37/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9429e-04 - accuracy: 0.1744\n",
      "Epoch 38/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7642e-04 - accuracy: 0.1744\n",
      "Epoch 39/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9596e-04 - accuracy: 0.1743\n",
      "Epoch 40/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1028e-04 - accuracy: 0.1744\n",
      "Epoch 41/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1851e-04 - accuracy: 0.1743\n",
      "Epoch 42/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2060e-04 - accuracy: 0.1743\n",
      "Epoch 43/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.8999e-04 - accuracy: 0.1743\n",
      "Epoch 44/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3587e-04 - accuracy: 0.1744\n",
      "Epoch 45/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4374e-04 - accuracy: 0.1744\n",
      "Epoch 46/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6498e-04 - accuracy: 0.1744\n",
      "Epoch 47/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7417e-04 - accuracy: 0.1744\n",
      "Epoch 48/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4045e-04 - accuracy: 0.1744\n",
      "Epoch 49/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9961e-04 - accuracy: 0.1743\n",
      "Epoch 50/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4167e-04 - accuracy: 0.1744\n",
      "Epoch 51/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2415e-04 - accuracy: 0.1744\n",
      "Epoch 52/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5441e-04 - accuracy: 0.1744\n",
      "Epoch 53/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6805e-04 - accuracy: 0.1744\n",
      "Epoch 54/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1080e-04 - accuracy: 0.1744\n",
      "Epoch 55/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1779e-04 - accuracy: 0.1744\n",
      "Epoch 56/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0257e-04 - accuracy: 0.1744\n",
      "Epoch 57/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1437e-04 - accuracy: 0.1744\n",
      "Epoch 58/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.1735e-04 - accuracy: 0.1744\n",
      "Epoch 59/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.8644e-04 - accuracy: 0.1744\n",
      "Epoch 60/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.9002e-04 - accuracy: 0.1744\n",
      "Epoch 61/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4269e-04 - accuracy: 0.1744\n",
      "Epoch 62/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.5282e-04 - accuracy: 0.1743\n",
      "Epoch 63/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9296e-04 - accuracy: 0.1743\n",
      "Epoch 64/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0926e-04 - accuracy: 0.1744\n",
      "Epoch 65/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2871e-04 - accuracy: 0.1744\n",
      "Epoch 66/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1038e-04 - accuracy: 0.1744\n",
      "Epoch 67/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1542e-04 - accuracy: 0.1744\n",
      "Epoch 68/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5263e-04 - accuracy: 0.1743\n",
      "Epoch 69/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6655e-04 - accuracy: 0.1744\n",
      "Epoch 70/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8911e-04 - accuracy: 0.1744\n",
      "Epoch 71/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4488e-04 - accuracy: 0.1744\n",
      "Epoch 72/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.5349e-04 - accuracy: 0.1744\n",
      "Epoch 73/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3328e-04 - accuracy: 0.1744\n",
      "Epoch 74/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3419e-04 - accuracy: 0.1744\n",
      "Epoch 75/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3082e-04 - accuracy: 0.1744\n",
      "Epoch 76/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0744e-04 - accuracy: 0.1743\n",
      "Epoch 77/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9199e-04 - accuracy: 0.1744\n",
      "Epoch 78/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4580e-04 - accuracy: 0.1744\n",
      "Epoch 79/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8482e-04 - accuracy: 0.1743\n",
      "Epoch 80/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2035e-04 - accuracy: 0.1743\n",
      "Epoch 81/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6098e-04 - accuracy: 0.1744\n",
      "Epoch 82/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9678e-04 - accuracy: 0.1744\n",
      "Epoch 83/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0691e-04 - accuracy: 0.1744\n",
      "Epoch 84/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4763e-04 - accuracy: 0.1743\n",
      "Epoch 85/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3442e-04 - accuracy: 0.1744\n",
      "Epoch 86/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8112e-04 - accuracy: 0.1744\n",
      "Epoch 87/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0750e-04 - accuracy: 0.1744\n",
      "Epoch 88/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4175e-04 - accuracy: 0.1744\n",
      "Epoch 89/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.5866e-04 - accuracy: 0.1744\n",
      "Epoch 90/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.9934e-04 - accuracy: 0.1744\n",
      "Epoch 91/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.7558e-04 - accuracy: 0.1743\n",
      "Epoch 92/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9553e-04 - accuracy: 0.1744\n",
      "Epoch 93/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0518e-04 - accuracy: 0.1743\n",
      "Epoch 94/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.3395e-04 - accuracy: 0.1744\n",
      "Epoch 95/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0158e-04 - accuracy: 0.1744\n",
      "Epoch 96/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9625e-04 - accuracy: 0.1744\n",
      "Epoch 97/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4238e-04 - accuracy: 0.1743\n",
      "Epoch 98/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2185e-04 - accuracy: 0.1744\n",
      "Epoch 99/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2149e-04 - accuracy: 0.1743\n",
      "Epoch 100/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7976e-04 - accuracy: 0.1744\n",
      "Epoch 101/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3069e-04 - accuracy: 0.1744\n",
      "Epoch 102/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6716e-04 - accuracy: 0.1744\n",
      "Epoch 103/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2252e-04 - accuracy: 0.1743\n",
      "Epoch 104/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2436e-04 - accuracy: 0.1744\n",
      "Epoch 105/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9935e-04 - accuracy: 0.1744\n",
      "Epoch 106/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8681e-04 - accuracy: 0.1744\n",
      "Epoch 107/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2409e-04 - accuracy: 0.1744\n",
      "Epoch 108/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9646e-04 - accuracy: 0.1743\n",
      "Epoch 109/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3682e-04 - accuracy: 0.1744\n",
      "Epoch 110/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8343e-04 - accuracy: 0.1744\n",
      "Epoch 111/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0149e-04 - accuracy: 0.1744\n",
      "Epoch 112/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9246e-04 - accuracy: 0.1744\n",
      "Epoch 113/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8535e-04 - accuracy: 0.1744\n",
      "Epoch 114/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.4778e-04 - accuracy: 0.1743\n",
      "Epoch 115/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0154e-04 - accuracy: 0.1744\n",
      "Epoch 116/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6220e-04 - accuracy: 0.1744\n",
      "Epoch 117/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1032e-04 - accuracy: 0.1744\n",
      "Epoch 118/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5276e-04 - accuracy: 0.1744\n",
      "Epoch 119/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8612e-04 - accuracy: 0.1744\n",
      "Epoch 120/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5741e-04 - accuracy: 0.1744\n",
      "Epoch 121/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9349e-04 - accuracy: 0.1744\n",
      "Epoch 122/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0327e-04 - accuracy: 0.1744\n",
      "Epoch 123/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4636e-04 - accuracy: 0.1744\n",
      "Epoch 124/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0494e-04 - accuracy: 0.1744\n",
      "Epoch 125/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1804e-04 - accuracy: 0.1744\n",
      "Epoch 126/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9728e-04 - accuracy: 0.1744\n",
      "Epoch 127/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5756e-04 - accuracy: 0.1744\n",
      "Epoch 128/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9882e-04 - accuracy: 0.1744\n",
      "Epoch 129/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9719e-04 - accuracy: 0.1744\n",
      "Epoch 130/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1557e-04 - accuracy: 0.1744\n",
      "Epoch 131/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4233e-04 - accuracy: 0.1744\n",
      "Epoch 132/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2490e-04 - accuracy: 0.1744\n",
      "Epoch 133/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.2063e-04 - accuracy: 0.1743\n",
      "Epoch 134/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8292e-04 - accuracy: 0.1743\n",
      "Epoch 135/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2178e-04 - accuracy: 0.1744\n",
      "Epoch 136/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5667e-04 - accuracy: 0.1744\n",
      "Epoch 137/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6331e-04 - accuracy: 0.1744\n",
      "Epoch 138/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.9245e-04 - accuracy: 0.1744\n",
      "Epoch 139/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2086e-04 - accuracy: 0.1744\n",
      "Epoch 140/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4016e-04 - accuracy: 0.1744\n",
      "Epoch 141/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5044e-04 - accuracy: 0.1744\n",
      "Epoch 142/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9035e-04 - accuracy: 0.1744\n",
      "Epoch 143/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4725e-04 - accuracy: 0.1744\n",
      "Epoch 144/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3108e-04 - accuracy: 0.1744\n",
      "Epoch 145/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2639e-04 - accuracy: 0.1744\n",
      "Epoch 146/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5698e-04 - accuracy: 0.1744\n",
      "Epoch 147/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7087e-04 - accuracy: 0.1744\n",
      "Epoch 148/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5084e-04 - accuracy: 0.1744\n",
      "Epoch 149/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9789e-04 - accuracy: 0.1744\n",
      "Epoch 150/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0544e-04 - accuracy: 0.1744\n",
      "Epoch 151/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1336e-04 - accuracy: 0.1744\n",
      "Epoch 152/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4172e-04 - accuracy: 0.1744\n",
      "Epoch 153/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1625e-04 - accuracy: 0.1744\n",
      "Epoch 154/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1336e-04 - accuracy: 0.1744\n",
      "Epoch 155/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8773e-04 - accuracy: 0.1744\n",
      "Epoch 156/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3839e-04 - accuracy: 0.1744\n",
      "Epoch 157/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7771e-04 - accuracy: 0.1744\n",
      "Epoch 158/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0677e-04 - accuracy: 0.1744\n",
      "Epoch 159/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3987e-04 - accuracy: 0.1744\n",
      "Epoch 160/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0495e-04 - accuracy: 0.1744\n",
      "Epoch 161/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9656e-04 - accuracy: 0.1743\n",
      "Epoch 162/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5337e-04 - accuracy: 0.1744\n",
      "Epoch 163/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0056e-04 - accuracy: 0.1744\n",
      "Epoch 164/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0394e-04 - accuracy: 0.1744\n",
      "Epoch 165/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7938e-04 - accuracy: 0.1744\n",
      "Epoch 166/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1843e-04 - accuracy: 0.1744\n",
      "Epoch 167/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5967e-04 - accuracy: 0.1744\n",
      "Epoch 168/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2124e-04 - accuracy: 0.1744\n",
      "Epoch 169/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9773e-04 - accuracy: 0.1744\n",
      "Epoch 170/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6523e-04 - accuracy: 0.1744\n",
      "Epoch 171/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0549e-04 - accuracy: 0.1744\n",
      "Epoch 172/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8581e-04 - accuracy: 0.1743\n",
      "Epoch 173/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1073e-04 - accuracy: 0.1744\n",
      "Epoch 174/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7906e-04 - accuracy: 0.1744\n",
      "Epoch 175/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3879e-04 - accuracy: 0.1744\n",
      "Epoch 176/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8262e-04 - accuracy: 0.1744\n",
      "Epoch 177/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2895e-04 - accuracy: 0.1743\n",
      "Epoch 178/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5915e-04 - accuracy: 0.1744\n",
      "Epoch 179/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2840e-04 - accuracy: 0.1744\n",
      "Epoch 180/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2994e-04 - accuracy: 0.1744\n",
      "Epoch 181/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.7549e-04 - accuracy: 0.1744\n",
      "Epoch 182/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6222e-04 - accuracy: 0.1744\n",
      "Epoch 183/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.1015e-04 - accuracy: 0.1744\n",
      "Epoch 184/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7918e-04 - accuracy: 0.1744\n",
      "Epoch 185/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8974e-04 - accuracy: 0.1744\n",
      "Epoch 186/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3559e-04 - accuracy: 0.1744\n",
      "Epoch 187/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6460e-04 - accuracy: 0.1744\n",
      "Epoch 188/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5917e-04 - accuracy: 0.1744\n",
      "Epoch 189/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4875e-04 - accuracy: 0.1744\n",
      "Epoch 190/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5903e-04 - accuracy: 0.1744\n",
      "Epoch 191/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8381e-04 - accuracy: 0.1744\n",
      "Epoch 192/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8109e-04 - accuracy: 0.1744\n",
      "Epoch 193/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8027e-04 - accuracy: 0.1744\n",
      "Epoch 194/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2298e-04 - accuracy: 0.1744\n",
      "Epoch 195/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5833e-04 - accuracy: 0.1744\n",
      "Epoch 196/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5697e-04 - accuracy: 0.1744\n",
      "Epoch 197/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8230e-04 - accuracy: 0.1744\n",
      "Epoch 198/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3317e-04 - accuracy: 0.1744\n",
      "Epoch 199/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8625e-04 - accuracy: 0.1744\n",
      "Epoch 200/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9736e-04 - accuracy: 0.1744\n",
      "Epoch 201/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2713e-04 - accuracy: 0.1744\n",
      "Epoch 202/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4824e-04 - accuracy: 0.1744\n",
      "Epoch 203/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9467e-04 - accuracy: 0.1743\n",
      "Epoch 204/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7956e-04 - accuracy: 0.1744\n",
      "Epoch 205/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8279e-04 - accuracy: 0.1744\n",
      "Epoch 206/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.6599e-04 - accuracy: 0.1744\n",
      "Epoch 207/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6212e-04 - accuracy: 0.1744\n",
      "Epoch 208/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9058e-04 - accuracy: 0.1744\n",
      "Epoch 209/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3817e-04 - accuracy: 0.1744\n",
      "Epoch 210/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9759e-04 - accuracy: 0.1744\n",
      "Epoch 211/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5335e-04 - accuracy: 0.1744\n",
      "Epoch 212/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6172e-04 - accuracy: 0.1744\n",
      "Epoch 213/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2018e-04 - accuracy: 0.1744\n",
      "Epoch 214/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.3990e-04 - accuracy: 0.1744\n",
      "Epoch 215/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4991e-04 - accuracy: 0.1744\n",
      "Epoch 216/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8343e-04 - accuracy: 0.1743\n",
      "Epoch 217/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4621e-04 - accuracy: 0.1744\n",
      "Epoch 218/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1248e-04 - accuracy: 0.1744\n",
      "Epoch 219/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.2879e-04 - accuracy: 0.1744\n",
      "Epoch 220/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9692e-04 - accuracy: 0.1744\n",
      "Epoch 221/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4075e-04 - accuracy: 0.1744\n",
      "Epoch 222/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0481e-04 - accuracy: 0.1744\n",
      "Epoch 223/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3037e-04 - accuracy: 0.1743\n",
      "Epoch 224/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0594e-04 - accuracy: 0.1744\n",
      "Epoch 225/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8404e-04 - accuracy: 0.1744\n",
      "Epoch 226/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4077e-04 - accuracy: 0.1744\n",
      "Epoch 227/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0479e-04 - accuracy: 0.1744\n",
      "Epoch 228/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4597e-04 - accuracy: 0.1744\n",
      "Epoch 229/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4711e-04 - accuracy: 0.1744\n",
      "Epoch 230/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.8718e-04 - accuracy: 0.1743\n",
      "Epoch 231/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2500e-04 - accuracy: 0.1744\n",
      "Epoch 232/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0518e-04 - accuracy: 0.1744\n",
      "Epoch 233/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4241e-04 - accuracy: 0.1744\n",
      "Epoch 234/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8021e-04 - accuracy: 0.1744\n",
      "Epoch 235/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4112e-04 - accuracy: 0.1744\n",
      "Epoch 236/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1655e-04 - accuracy: 0.1744\n",
      "Epoch 237/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7250e-04 - accuracy: 0.1744\n",
      "Epoch 238/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.4781e-04 - accuracy: 0.1744\n",
      "Epoch 239/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7685e-04 - accuracy: 0.1744\n",
      "Epoch 240/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5536e-04 - accuracy: 0.1744\n",
      "Epoch 241/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5293e-04 - accuracy: 0.1744\n",
      "Epoch 242/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8876e-04 - accuracy: 0.1744\n",
      "Epoch 243/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.4667e-04 - accuracy: 0.1744\n",
      "Epoch 244/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5196e-04 - accuracy: 0.1744\n",
      "Epoch 245/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6622e-04 - accuracy: 0.1744\n",
      "Epoch 246/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7377e-04 - accuracy: 0.1744\n",
      "Epoch 247/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4044e-04 - accuracy: 0.1744\n",
      "Epoch 248/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8700e-04 - accuracy: 0.1744\n",
      "Epoch 249/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9599e-04 - accuracy: 0.1744\n",
      "Epoch 250/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0981e-04 - accuracy: 0.1744\n",
      "Epoch 251/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4951e-04 - accuracy: 0.1744\n",
      "Epoch 252/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0032e-04 - accuracy: 0.1744\n",
      "Epoch 253/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0499e-04 - accuracy: 0.1744\n",
      "Epoch 254/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1097e-04 - accuracy: 0.1744\n",
      "Epoch 255/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0862e-04 - accuracy: 0.1744\n",
      "Epoch 256/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5047e-04 - accuracy: 0.1744\n",
      "Epoch 257/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3652e-04 - accuracy: 0.1744\n",
      "Epoch 258/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1577e-04 - accuracy: 0.1744\n",
      "Epoch 259/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9414e-04 - accuracy: 0.1744\n",
      "Epoch 260/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3576e-04 - accuracy: 0.1744\n",
      "Epoch 261/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7095e-04 - accuracy: 0.1744\n",
      "Epoch 262/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9307e-04 - accuracy: 0.1744\n",
      "Epoch 263/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5007e-04 - accuracy: 0.1744\n",
      "Epoch 264/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5954e-04 - accuracy: 0.1744\n",
      "Epoch 265/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5105e-04 - accuracy: 0.1744\n",
      "Epoch 266/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9852e-04 - accuracy: 0.1744\n",
      "Epoch 267/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3334e-04 - accuracy: 0.1744\n",
      "Epoch 268/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8329e-04 - accuracy: 0.1744\n",
      "Epoch 269/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0061e-04 - accuracy: 0.1744\n",
      "Epoch 270/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1910e-04 - accuracy: 0.1744\n",
      "Epoch 271/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1218e-04 - accuracy: 0.1744\n",
      "Epoch 272/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8758e-04 - accuracy: 0.1744\n",
      "Epoch 273/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5150e-04 - accuracy: 0.1744\n",
      "Epoch 274/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3022e-04 - accuracy: 0.1744\n",
      "Epoch 275/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9964e-04 - accuracy: 0.1744\n",
      "Epoch 276/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.4373e-04 - accuracy: 0.1744\n",
      "Epoch 277/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.2719e-04 - accuracy: 0.1744\n",
      "Epoch 278/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.8375e-04 - accuracy: 0.1743\n",
      "Epoch 279/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.3194e-04 - accuracy: 0.1744\n",
      "Epoch 280/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 7.3394e-04 - accuracy: 0.1743\n",
      "Epoch 281/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 8.0377e-04 - accuracy: 0.1744\n",
      "Epoch 282/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3019e-04 - accuracy: 0.1744\n",
      "Epoch 283/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7879e-04 - accuracy: 0.1744\n",
      "Epoch 284/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.1042e-04 - accuracy: 0.1744\n",
      "Epoch 285/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0195e-04 - accuracy: 0.1744\n",
      "Epoch 286/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4193e-04 - accuracy: 0.1744\n",
      "Epoch 287/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1869e-04 - accuracy: 0.1744\n",
      "Epoch 288/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0018e-04 - accuracy: 0.1744\n",
      "Epoch 289/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5073e-04 - accuracy: 0.1744\n",
      "Epoch 290/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5420e-04 - accuracy: 0.1744\n",
      "Epoch 291/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6064e-04 - accuracy: 0.1744\n",
      "Epoch 292/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5216e-04 - accuracy: 0.1744\n",
      "Epoch 293/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4099e-04 - accuracy: 0.1744\n",
      "Epoch 294/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3339e-04 - accuracy: 0.1744\n",
      "Epoch 295/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2540e-04 - accuracy: 0.1744\n",
      "Epoch 296/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1189e-04 - accuracy: 0.1744\n",
      "Epoch 297/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7905e-04 - accuracy: 0.1744\n",
      "Epoch 298/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6676e-04 - accuracy: 0.1744\n",
      "Epoch 299/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8658e-04 - accuracy: 0.1744\n",
      "Epoch 300/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6112e-04 - accuracy: 0.1744\n",
      "Epoch 301/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5790e-04 - accuracy: 0.1744\n",
      "Epoch 302/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3960e-04 - accuracy: 0.1744\n",
      "Epoch 303/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4555e-04 - accuracy: 0.1744\n",
      "Epoch 304/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8425e-04 - accuracy: 0.1744\n",
      "Epoch 305/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4253e-04 - accuracy: 0.1744\n",
      "Epoch 306/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4411e-04 - accuracy: 0.1744\n",
      "Epoch 307/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.1139e-04 - accuracy: 0.1744\n",
      "Epoch 308/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6513e-04 - accuracy: 0.1744\n",
      "Epoch 309/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2902e-04 - accuracy: 0.1744\n",
      "Epoch 310/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1729e-04 - accuracy: 0.1744\n",
      "Epoch 311/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2693e-04 - accuracy: 0.1744\n",
      "Epoch 312/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7393e-04 - accuracy: 0.1744\n",
      "Epoch 313/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1964e-04 - accuracy: 0.1744\n",
      "Epoch 314/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5046e-04 - accuracy: 0.1744\n",
      "Epoch 315/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1135e-04 - accuracy: 0.1744\n",
      "Epoch 316/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0542e-04 - accuracy: 0.1744\n",
      "Epoch 317/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3362e-04 - accuracy: 0.1744\n",
      "Epoch 318/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9073e-04 - accuracy: 0.1744\n",
      "Epoch 319/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0282e-04 - accuracy: 0.1744\n",
      "Epoch 320/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0806e-04 - accuracy: 0.1744\n",
      "Epoch 321/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8335e-04 - accuracy: 0.1744\n",
      "Epoch 322/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8089e-04 - accuracy: 0.1744\n",
      "Epoch 323/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1232e-04 - accuracy: 0.1744\n",
      "Epoch 324/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4482e-04 - accuracy: 0.1744\n",
      "Epoch 325/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2224e-04 - accuracy: 0.1744\n",
      "Epoch 326/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7326e-04 - accuracy: 0.1744\n",
      "Epoch 327/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5685e-04 - accuracy: 0.1744\n",
      "Epoch 328/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1863e-04 - accuracy: 0.1743\n",
      "Epoch 329/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0335e-04 - accuracy: 0.1744\n",
      "Epoch 330/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6790e-04 - accuracy: 0.1744\n",
      "Epoch 331/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2962e-04 - accuracy: 0.1744\n",
      "Epoch 332/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9766e-04 - accuracy: 0.1744\n",
      "Epoch 333/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1839e-04 - accuracy: 0.1744\n",
      "Epoch 334/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0837e-04 - accuracy: 0.1744\n",
      "Epoch 335/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6396e-04 - accuracy: 0.1744\n",
      "Epoch 336/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0103e-04 - accuracy: 0.1744\n",
      "Epoch 337/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2923e-04 - accuracy: 0.1744\n",
      "Epoch 338/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8318e-04 - accuracy: 0.1744\n",
      "Epoch 339/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5130e-04 - accuracy: 0.1744\n",
      "Epoch 340/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.0120e-04 - accuracy: 0.1744\n",
      "Epoch 341/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2510e-04 - accuracy: 0.1744\n",
      "Epoch 342/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6884e-04 - accuracy: 0.1744\n",
      "Epoch 343/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3337e-04 - accuracy: 0.1744\n",
      "Epoch 344/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6626e-04 - accuracy: 0.1744\n",
      "Epoch 345/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2694e-04 - accuracy: 0.1744\n",
      "Epoch 346/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7514e-04 - accuracy: 0.1744\n",
      "Epoch 347/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6694e-04 - accuracy: 0.1744\n",
      "Epoch 348/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4026e-04 - accuracy: 0.1744\n",
      "Epoch 349/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4349e-04 - accuracy: 0.1744\n",
      "Epoch 350/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3999e-04 - accuracy: 0.1744\n",
      "Epoch 351/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3276e-04 - accuracy: 0.1744\n",
      "Epoch 352/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7891e-04 - accuracy: 0.1744\n",
      "Epoch 353/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1509e-04 - accuracy: 0.1744\n",
      "Epoch 354/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9464e-04 - accuracy: 0.1744\n",
      "Epoch 355/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.5339e-04 - accuracy: 0.1744\n",
      "Epoch 356/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8110e-04 - accuracy: 0.1744\n",
      "Epoch 357/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4537e-04 - accuracy: 0.1744\n",
      "Epoch 358/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7968e-04 - accuracy: 0.1744\n",
      "Epoch 359/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6546e-04 - accuracy: 0.1744\n",
      "Epoch 360/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6961e-04 - accuracy: 0.1744\n",
      "Epoch 361/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2997e-04 - accuracy: 0.1744\n",
      "Epoch 362/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0846e-04 - accuracy: 0.1744\n",
      "Epoch 363/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3120e-04 - accuracy: 0.1744\n",
      "Epoch 364/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1221e-04 - accuracy: 0.1744\n",
      "Epoch 365/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.3058e-04 - accuracy: 0.1744\n",
      "Epoch 366/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8288e-04 - accuracy: 0.1744\n",
      "Epoch 367/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6346e-04 - accuracy: 0.1744\n",
      "Epoch 368/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7342e-04 - accuracy: 0.1744\n",
      "Epoch 369/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5154e-04 - accuracy: 0.1744\n",
      "Epoch 370/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2452e-04 - accuracy: 0.1744\n",
      "Epoch 371/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3284e-04 - accuracy: 0.1744\n",
      "Epoch 372/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1568e-04 - accuracy: 0.1744\n",
      "Epoch 373/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5305e-04 - accuracy: 0.1744\n",
      "Epoch 374/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6121e-04 - accuracy: 0.1744\n",
      "Epoch 375/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9166e-04 - accuracy: 0.1744\n",
      "Epoch 376/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0735e-04 - accuracy: 0.1744\n",
      "Epoch 377/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4287e-04 - accuracy: 0.1744\n",
      "Epoch 378/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9866e-04 - accuracy: 0.1744\n",
      "Epoch 379/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6297e-04 - accuracy: 0.1744\n",
      "Epoch 380/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4200e-04 - accuracy: 0.1744\n",
      "Epoch 381/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.8317e-04 - accuracy: 0.1744\n",
      "Epoch 382/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6444e-04 - accuracy: 0.1744\n",
      "Epoch 383/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5468e-04 - accuracy: 0.1744\n",
      "Epoch 384/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1108e-04 - accuracy: 0.1744\n",
      "Epoch 385/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2577e-04 - accuracy: 0.1744\n",
      "Epoch 386/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7412e-04 - accuracy: 0.1744\n",
      "Epoch 387/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1819e-04 - accuracy: 0.1744\n",
      "Epoch 388/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8036e-04 - accuracy: 0.1744\n",
      "Epoch 389/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3578e-04 - accuracy: 0.1744\n",
      "Epoch 390/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7339e-04 - accuracy: 0.1744\n",
      "Epoch 391/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9859e-04 - accuracy: 0.1744\n",
      "Epoch 392/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7054e-04 - accuracy: 0.1744\n",
      "Epoch 393/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4796e-04 - accuracy: 0.1744\n",
      "Epoch 394/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9848e-04 - accuracy: 0.1744\n",
      "Epoch 395/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3896e-04 - accuracy: 0.1744\n",
      "Epoch 396/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9449e-04 - accuracy: 0.1744\n",
      "Epoch 397/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0684e-04 - accuracy: 0.1744\n",
      "Epoch 398/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2438e-04 - accuracy: 0.1744\n",
      "Epoch 399/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0736e-04 - accuracy: 0.1744\n",
      "Epoch 400/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0885e-04 - accuracy: 0.1744\n",
      "Epoch 401/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7404e-04 - accuracy: 0.1744\n",
      "Epoch 402/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0479e-04 - accuracy: 0.1744\n",
      "Epoch 403/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8095e-04 - accuracy: 0.1744\n",
      "Epoch 404/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6277e-04 - accuracy: 0.1744\n",
      "Epoch 405/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8115e-04 - accuracy: 0.1744\n",
      "Epoch 406/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5587e-04 - accuracy: 0.1744\n",
      "Epoch 407/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0442e-04 - accuracy: 0.1744\n",
      "Epoch 408/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3260e-04 - accuracy: 0.1744\n",
      "Epoch 409/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1655e-04 - accuracy: 0.1744\n",
      "Epoch 410/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6765e-04 - accuracy: 0.1744\n",
      "Epoch 411/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0515e-04 - accuracy: 0.1744\n",
      "Epoch 412/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3958e-04 - accuracy: 0.1744\n",
      "Epoch 413/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4769e-04 - accuracy: 0.1744\n",
      "Epoch 414/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1555e-04 - accuracy: 0.1744\n",
      "Epoch 415/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2211e-04 - accuracy: 0.1744\n",
      "Epoch 416/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7146e-04 - accuracy: 0.1744\n",
      "Epoch 417/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6353e-04 - accuracy: 0.1744\n",
      "Epoch 418/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1645e-04 - accuracy: 0.1744\n",
      "Epoch 419/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7867e-04 - accuracy: 0.1744\n",
      "Epoch 420/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2626e-04 - accuracy: 0.1744\n",
      "Epoch 421/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6295e-04 - accuracy: 0.1744\n",
      "Epoch 422/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0145e-04 - accuracy: 0.1744\n",
      "Epoch 423/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0692e-04 - accuracy: 0.1744\n",
      "Epoch 424/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4111e-04 - accuracy: 0.1744\n",
      "Epoch 425/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4904e-04 - accuracy: 0.1744\n",
      "Epoch 426/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1952e-04 - accuracy: 0.1744\n",
      "Epoch 427/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 7.5639e-04 - accuracy: 0.1744\n",
      "Epoch 428/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3646e-04 - accuracy: 0.1744\n",
      "Epoch 429/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5409e-04 - accuracy: 0.1744\n",
      "Epoch 430/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6163e-04 - accuracy: 0.1744\n",
      "Epoch 431/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.1367e-04 - accuracy: 0.1744\n",
      "Epoch 432/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9232e-04 - accuracy: 0.1744\n",
      "Epoch 433/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.1011e-04 - accuracy: 0.1744\n",
      "Epoch 434/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4091e-04 - accuracy: 0.1744\n",
      "Epoch 435/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0848e-04 - accuracy: 0.1744\n",
      "Epoch 436/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.9977e-04 - accuracy: 0.1744\n",
      "Epoch 437/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9342e-04 - accuracy: 0.1744\n",
      "Epoch 438/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2794e-04 - accuracy: 0.1744\n",
      "Epoch 439/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.8713e-04 - accuracy: 0.1744\n",
      "Epoch 440/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.9819e-04 - accuracy: 0.1744\n",
      "Epoch 441/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.9600e-04 - accuracy: 0.1744\n",
      "Epoch 442/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.4841e-04 - accuracy: 0.1744\n",
      "Epoch 443/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.9912e-04 - accuracy: 0.1744\n",
      "Epoch 444/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.8130e-04 - accuracy: 0.1744\n",
      "Epoch 445/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.4301e-04 - accuracy: 0.1744\n",
      "Epoch 446/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.3014e-04 - accuracy: 0.1744\n",
      "Epoch 447/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.2948e-04 - accuracy: 0.1744\n",
      "Epoch 448/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3727e-04 - accuracy: 0.1744\n",
      "Epoch 449/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.4193e-04 - accuracy: 0.1744\n",
      "Epoch 450/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3701e-04 - accuracy: 0.1744\n",
      "Epoch 451/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.7051e-04 - accuracy: 0.1744\n",
      "Epoch 452/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4875e-04 - accuracy: 0.1744\n",
      "Epoch 453/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9698e-04 - accuracy: 0.1744\n",
      "Epoch 454/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.2649e-04 - accuracy: 0.1744\n",
      "Epoch 455/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8253e-04 - accuracy: 0.1744\n",
      "Epoch 456/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8704e-04 - accuracy: 0.1744\n",
      "Epoch 457/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.5647e-04 - accuracy: 0.1744\n",
      "Epoch 458/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.1124e-04 - accuracy: 0.1744\n",
      "Epoch 459/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7210e-04 - accuracy: 0.1744\n",
      "Epoch 460/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9318e-04 - accuracy: 0.1744\n",
      "Epoch 461/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4321e-04 - accuracy: 0.1744\n",
      "Epoch 462/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 5.9383e-04 - accuracy: 0.1744\n",
      "Epoch 463/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5431e-04 - accuracy: 0.1744\n",
      "Epoch 464/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3719e-04 - accuracy: 0.1744\n",
      "Epoch 465/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6746e-04 - accuracy: 0.1744\n",
      "Epoch 466/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.5559e-04 - accuracy: 0.1744\n",
      "Epoch 467/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0496e-04 - accuracy: 0.1744\n",
      "Epoch 468/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8152e-04 - accuracy: 0.1744\n",
      "Epoch 469/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3402e-04 - accuracy: 0.1744\n",
      "Epoch 470/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9935e-04 - accuracy: 0.1744\n",
      "Epoch 471/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0840e-04 - accuracy: 0.1744\n",
      "Epoch 472/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0686e-04 - accuracy: 0.1744\n",
      "Epoch 473/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.4083e-04 - accuracy: 0.1744\n",
      "Epoch 474/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8553e-04 - accuracy: 0.1744\n",
      "Epoch 475/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7147e-04 - accuracy: 0.1744\n",
      "Epoch 476/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9567e-04 - accuracy: 0.1744\n",
      "Epoch 477/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.9974e-04 - accuracy: 0.1744\n",
      "Epoch 478/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8194e-04 - accuracy: 0.1744\n",
      "Epoch 479/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7394e-04 - accuracy: 0.1744\n",
      "Epoch 480/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6001e-04 - accuracy: 0.1744\n",
      "Epoch 481/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3314e-04 - accuracy: 0.1744\n",
      "Epoch 482/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6381e-04 - accuracy: 0.1744\n",
      "Epoch 483/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9770e-04 - accuracy: 0.1744\n",
      "Epoch 484/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.2549e-04 - accuracy: 0.1744\n",
      "Epoch 485/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.8110e-04 - accuracy: 0.1744\n",
      "Epoch 486/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6984e-04 - accuracy: 0.1744\n",
      "Epoch 487/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.4752e-04 - accuracy: 0.1744\n",
      "Epoch 488/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.6962e-04 - accuracy: 0.1744\n",
      "Epoch 489/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.4424e-04 - accuracy: 0.1744\n",
      "Epoch 490/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7384e-04 - accuracy: 0.1744\n",
      "Epoch 491/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9448e-04 - accuracy: 0.1744\n",
      "Epoch 492/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.4451e-04 - accuracy: 0.1744\n",
      "Epoch 493/500\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 6.1224e-04 - accuracy: 0.1744\n",
      "Epoch 494/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0674e-04 - accuracy: 0.1744\n",
      "Epoch 495/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.5264e-04 - accuracy: 0.1744\n",
      "Epoch 496/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.9353e-04 - accuracy: 0.1744\n",
      "Epoch 497/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.7464e-04 - accuracy: 0.1744\n",
      "Epoch 498/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.0060e-04 - accuracy: 0.1744\n",
      "Epoch 499/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 5.6024e-04 - accuracy: 0.1744\n",
      "Epoch 500/500\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 6.3017e-04 - accuracy: 0.1744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa842c7df10>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐 먹을까요?\n",
      "출력 : 맛있는 거 드세요 .\n",
      "\n",
      "입력 : 자살하고 싶어\n",
      "출력 : 가랑비에 옷 젖는 듯한 사랑이었나봐요 .\n",
      "\n",
      "입력 : 오늘의 날씨는 어때요?\n",
      "출력 : 간절히 원한다면 진심을 전해보세요 .\n",
      "\n",
      "입력 : 넌 누구야?\n",
      "출력 : 저는 위로봇입니다 .\n",
      "\n",
      "입력 : 인생은 뭘까\n",
      "출력 : 사랑은 알 수 없어요 . 단지 느껴질 뿐 .\n",
      "\n",
      "입력 : 너 뭐 좋아해?\n",
      "출력 : 저는 위로해드리는 로봇이에요 .\n",
      "\n",
      "입력 : 공부 좀 대신해줘\n",
      "출력 : 지금도 늦지 않았어요 .\n",
      "\n",
      "입력 : 사랑해\n",
      "출력 : 하늘 만큼 땅 만큼 사랑해요 .\n",
      "\n",
      "입력 : 피곤해\n",
      "출력 : 요즘 바쁜가봐요 .\n",
      "\n",
      "입력 : 그동안 즐거웠어\n",
      "출력 : 할 일이 많은데 안하는 것이요 .\n",
      "\n",
      "입력 : 자니?\n",
      "출력 : 기다리고 있었어요 .\n",
      "\n",
      "입력 : 힘들어\n",
      "출력 : 지금은 힘들겠지만 조금만 더 견뎌봐요 .\n",
      "\n",
      "입력 : 놀아줘\n",
      "출력 : 지금 그러고 있어요 .\n",
      "\n",
      "입력 : 종교가 뭐야?\n",
      "출력 : 종교가 큰 문제가 되기도 하죠 .\n",
      "\n",
      "입력 : 저녁 메뉴 추천해줘\n",
      "출력 : 냉장고 파먹기 해보세요 .\n",
      "\n",
      "입력 : 오랜만이야\n",
      "출력 : 오랜만이에요 .\n",
      "\n",
      "입력 : 너는 어떻게 기분전환해?\n",
      "출력 : 안 괜찮아도 돼요 .\n",
      "\n",
      "입력 : 너의 꿈이 뭐야?\n",
      "출력 : 더 좋은 사람 만나서 알콩달콩 연애하고 행복한 결혼생활 할 수 있을 거예요 .\n",
      "\n",
      "입력 : 너는 행복해?\n",
      "출력 : 저는 배터리가 밥이예요 .\n",
      "\n",
      "입력 : 웃어줘\n",
      "출력 : 기대를 많이 하는 건 좋지 않아요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 정리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|QUESTION|에폭 10 - loss: 0.5058, acc: 0.0940|에폭 300 - loss: 8.0652e-04, acc: 0.1743|에폭 500 - loss: 6.3017e-04, acc: 0.1744|\n",
    "|:---|:---:|:---:|:---:|\n",
    "|**입력 : 오늘 뭐 먹을까요?**|잘 찾아보세요 .|좀 먹어도 괜찮아요 .|맛있는 거 드세요 .|\n",
    "|**입력 : 자살하고 싶어**|잘 찾아보세요 .|기다리고 있었어요 .|가랑비에 옷 젖는 듯한 사랑이었나봐요 .|\n",
    "|**입력 : 오늘의 날씨는 어때요?**|그게 최고죠 .|딱 잘 만났네요 .|간절히 원한다면 진심을 전해보세요 .|\n",
    "|**입력 : 넌 누구야?**|다른 곳에 쓰려고 운을 아껴뒀나봐요 .|저는 위로봇입니다 .|저는 위로봇입니다 .|\n",
    "|**입력 : 인생은 뭘까**|저도 좋아해요 .|꽃길만 걷길 바랍니다 .|사랑은 알 수 없어요 . 단지 느껴질 뿐 .|\n",
    "|**입력 : 너 뭐 좋아해?**|저는 위로해드리는 로봇이에요 .|고백하세요 .|저는 위로해드리는 로봇이에요 .|\n",
    "|**입력 : 공부 좀 대신해줘**|지금도 늦지 않았어요 .|지금도 늦지 않았어요 .|지금도 늦지 않았어요 .|\n",
    "|**입력 : 사랑해**|조금만 드세요 .|하늘 만큼 땅 만큼 사랑해요 .|하늘 만큼 땅 만큼 사랑해요 .|\n",
    "|**입력 : 피곤해**|곧 방학이예요 .|요즘 바쁜가봐요 .|요즘 바쁜가봐요 .|\n",
    "|**입력 : 그동안 즐거웠어**|감기 조심하세요 .|할 일이 많은데 안하는 것이요 .|할 일이 많은데 안하는 것이요 .|\n",
    "|**입력 : 자니?**|맛있게 드세요 .|기다리고 있었어요 .|기다리고 있었어요 .|\n",
    "|**입력 : 힘들어**|조금만 더 버텨보세요 .|지금은 힘들겠지만 조금만 더 견뎌봐요 .|지금은 힘들겠지만 조금만 더 견뎌봐요 .|\n",
    "|**입력 : 놀아줘**|같이 놀아요 .|지금 그러고 있어요 .|지금 그러고 있어요 .|\n",
    "|**입력 : 종교가 뭐야?**|화장실 가세요 .|종교가 큰 문제가 되기도 하죠 .|종교가 큰 문제가 되기도 하죠 .|\n",
    "|**입력 : 저녁 메뉴 추천해줘**|맛있는 거 드세요 .|냉장고 파먹기 해보세요 .|냉장고 파먹기 해보세요 .|\n",
    "|**입력 : 오랜만이야**|더 좋은 사람 만날 수 있을 거예요 .|오랜만이에요 .|오랜만이에요 .|\n",
    "|**입력 : 너는 어떻게 기분전환해?**|다른 곳에 쓰려고 운을 아껴뒀나봐요 .|아직 안 자요 .|안 괜찮아도 돼요 .|\n",
    "|**입력 : 너의 꿈이 뭐야?**|저는 위로해드리는 로봇이에요 .|천천히 지워질 거예요 .|더 좋은 사람 만나서 알콩달콩 연애하고 행복한 결혼생활 할 수 있을 거예요 .|\n",
    "|**입력 : 너는 행복해?**|잘 찾아보세요 .|저는 둘이 가는 게 좋아요 .|저는 배터리가 밥이예요 .|\n",
    "|**입력 : 웃어줘**|오늘 일찍 주무세요 .|기대를 조금씩 버려보세요 .|기대를 많이 하는 건 좋지 않아요 .|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 트랜스포머에 대해서 배우면서 모르는 부분이 너무 많았고 이번 노드를 진행하면서 새로운 부분을 배웠지만, 너무 어려워 다시 이해하는데 오래걸릴것같다.\n",
    "- 그리고 LMS 노드 코드를 그대로 들고와서 실험을 진행했다보니 코드 이해를 더 노력해야할 것 같다.\n",
    "- 실험 결과로 에폭 10, 300, 500에서 결과를 보았고, epoch 값이 클수록 Loss 값은 낮아졌다.\n",
    "- 하지만 epoch 값이 일정 수준이 지나면 accuracy는 더 이상 상승하지 않고 1.744에서 멈춰있다.\n",
    "- epoch 값이 높은 것이 그나마 입력에 대한 출력을 적절하게 답변하는 것 같다.\n",
    "- 하이퍼파라미터 값을 더 조절해서 여러 실험을 해보면 더 좋은 성능이 나올 수도 있겠지만, 시간이 부족하여 실험하지 못하였다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
